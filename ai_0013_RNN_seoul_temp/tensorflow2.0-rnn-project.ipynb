{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRVS1oB2be3P"
   },
   "source": [
    "# boostcourse_tensorflow2.0_rnn_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnSIyzOdbe3R"
   },
   "source": [
    "### 실습 전 안내\n",
    "* 실습코드는 <font color='red'>Python 3.6</font>, <font color='red'>TensorFlow 2.0</font>버전을 기준으로 작성되었습니다. 원활한 실습 진행을 위해 버전을 맞춰주시는 것을 권장드립니다.\n",
    "* 간혹 컴퓨터 성능에 따라 모델을 학습시키는 과정에서 시간이 소요될 수 있습니다. 이 경우 <font color='red'>Colab</font>으로 실습을 진행해주시는 것을 권장드립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlwJkGprbe3S"
   },
   "source": [
    "## Colab 사용자를 위한 안내\n",
    "\n",
    "해당 노트북은 **로컬** 환경에서 최적화 되어 있습니다. **로컬 환경에서 진행하시는 분**들은 바로 학습을 진행하시면 됩니다.\n",
    "\n",
    "**Colab을 사용하시는 분**들은 처음에 아래 주석을 해제하시고 한번 만 실행시켜주세요!\n",
    "\n",
    "* 주석을 해제하는 방법: 해당 영역을 선택하고, `Ctrl + /` 를 누르면 해당 영역의 주석에 해제됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1583156248682,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "EjX7Oi7Ybe3T",
    "outputId": "9f23d50e-33d6-49cf-a9a3-0774579df5b6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGtGNTmCbe3W"
   },
   "source": [
    "Colab 을 사용하시는 분들은 아래 주석을 해제하시고 `folder` 변수 명에 프로젝트 디렉토리를 저장한 위치를 작성해주세요! 예를 들어, `03_rnn_tf` 의 위치가 \"내 드라이브 > colab_notebook > tensorflow\" 폴더 안에 있는 경우, \"colab_notebook/tensorflow\" 를 작성하시면 됩니다.\n",
    "\n",
    "```python\n",
    "folder = \"colab_notebook/tensorflow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1583156248683,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "jNh33WVkbe3X",
    "outputId": "d8f5eca0-27c4-44be-f12e-b9f65a61928b"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # folder 변수에 구글드라이브에 프로젝트를 저장한 디렉토리를 입력하세요!\n",
    "# # My Drive 밑에 저장했다면 그대로 두시면 됩니다.\n",
    "# folder = \"\"\n",
    "# project_dir = \"03_rnn_tf\"\n",
    "\n",
    "# base_path = Path(\"/content/drive/My Drive/\")\n",
    "# project_path = base_path / folder / project_dir\n",
    "# os.chdir(project_path)\n",
    "# for x in list(project_path.glob(\"*\")):\n",
    "#     if x.is_dir():\n",
    "#         dir_name = str(x.relative_to(project_path))\n",
    "#         os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
    "# print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wrMA0QDbmbQ"
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xmdEdHzbe3a"
   },
   "source": [
    "# Recurrent Neural Network: Neural Weather Forecaster\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=139sr_X8Hi8O8s43syKDxW7UCF33X_mjh\" width=\"600px\" height=\"800px\" />\n",
    "\n",
    "* 이미지 출처: 네이버\n",
    "\n",
    "많은 사람이 아침에 집을 나서기 전에 오늘 기온이 어떤지 혹은 비가 오는지 알기 위해 일기예보를 확인합니다. 그런데 혹시 일기예보가 어떻게 이루어지는지 생각해 보신적이 있으신가요? 아직 오지도 않은 미래의 날씨를 어떻게 예측할 수 있을까요? 아마 여러분 대부분은 기상 예측과 관련된 전문적인 지식에 대해 잘 알지 못할 것입니다. 엄청난 계산능력을 갖춘 슈퍼컴퓨터가 복잡한 계산을 통해 예측을 한다는 정도는 들어보신 분들도 계실 수 있겠네요. 하지만 딥러닝을 활용할 수 있고, 지난 과거의 기후 데이터만 가지고 있으면 여러분의 PC에서도 훌륭한 일기예보 모델을 학습시킬 수 있습니다. 이번 프로젝트에서는 RNN을 직접 설계하여 24시간 후의 기온을 예측하는 문제를 해결할 것입니다.\n",
    "\n",
    "\n",
    "이번 실습의 목표는 다음과 같습니다.\n",
    "- RNN을 설계하여 지난 며칠 동안의 날씨 정보를 기반으로 24시간 이후의 기온을 예측한다.\n",
    "- 다양한 속성의 시계열 정보를 활용하기 위해 적절한 전처리 과정을 적용한다. \n",
    "- 설계한 모델의 성능을 검증하기 위해 베이스라인 모델을 도입한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AJYMBh8be3b"
   },
   "source": [
    "실습코드는 Python 3.6, TensorFlow 2.0.0 버전을 기준으로 작성되었습니다.\n",
    "\n",
    "이번 과정을 통해 얻는 최종 결과물은 아래 그림과 같습니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UD4n1qLY2o3ayAQq-LOOiHd76qiCVAYk\" width=\"600px\" height=\"400px\" />\n",
    "<center>&lt;기온, 강수량 등 다양한 정보로 미래의 기온 예측&gt;</center></caption>\n",
    "\n",
    "### 이제부터 본격적으로 프로젝트를 시작하겠습니다.\n",
    "\n",
    "**\"[TODO] 코드 구현\"** 부분의 **\"## 코드 시작 ##\"** 부터 **\"## 코드 종료 ##\"** 구간에 필요한 코드를 작성해주세요. **나머지 작성구간이 명시 되지 않은 구간은 임의로 수정하지 마세요!**\n",
    "\n",
    "**본문 중간중간에 TensorFlow 함수들에 대해 [TensorFlow API 문서](https://www.tensorflow.org/api_docs/python/tf) 링크를 걸어두었습니다. API 문서를 직접 확인하는 일에 익숙해지면 나중에 여러분이 처음부터 모델을 직접 구현해야 할 때 정말 큰 도움이 됩니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JO_EaOmfbe3c"
   },
   "source": [
    "## 1. Package load\n",
    "\n",
    "필요한 패키지들을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3123,
     "status": "ok",
     "timestamp": 1583156250789,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "Gj7Z6vQWbe3d",
    "outputId": "982a80d3-5d07-4dbb-8d8f-47881bcb90a0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import check_util.checker as checker\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7r_9epQ2be3h"
   },
   "source": [
    "## 2. 하이퍼파라미터 세팅\n",
    "\n",
    "학습에 필요한 하이퍼파리미터의 값을 초기화해줍니다.\n",
    "\n",
    "미니배치의 크기(`batch_size`), 학습 할 epoch 수(`max_epochs`), 학습률(`learning_rate`) 등의 값들을 다음과 같이 정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGQNwHA3be3i"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 30\n",
    "learning_rate = 3e-5\n",
    "hidden_sizes = [100, 100] # hidden_sizes must be a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cofC8oUAbe3l"
   },
   "source": [
    "## 3. 데이터 전처리 함수 정의\n",
    "\n",
    "우리는 이번 실습에서 지난 10년간의(2009년~2018년) 서울시 기후 데이터를 활용해 기온을 예측하는 모델을 학습시킬 것입니다. 데이터셋은 [기상자료개방포털](https://data.kma.go.kr/)에서 받은 자료입니다. 이번 실습에서 사용하는 데이터 이외에도 기상자료개방포털에서 기상과 관련된 다양한 자료들을 내려받으실 수 있습니다. \n",
    "\n",
    "`./data/climate_seoul` 경로의 디렉토리를 보시면, `test` / `train` / `val` 디렉토리에 csv파일이 각각 1개 / 8개/ 1개 담겨있음을 확인하실 수 있습니다. 각 csv파일은 1년간의 서울시 기후 데이터를 담고 있으며, 1시간 간격으로 기록된 정보입니다. (사실 아주 가끔씩 30분 간격으로 기록한 구간도 있기도 하지만, 이후 본문에서는 편의상 모두 1시간 간격으로 기록된 것으로 간주하겠습니다.) 매 시간마다 기록되는 정보는 기온, 강수량, 풍속 등을 포함한 총 25가지 속성으로 이루어져 있습니다. 이 중에서 우리는 기온, 강수량, 풍속, 습도, 증기압을 포함한 총 9가지의 속성만을 사용하여 기온 예측 모델을 학습시켜 보겠습니다.\n",
    "\n",
    "그렇다면 왜 하필 이 9가지의 속성을 선택한 것일까요? 이렇게 9가지의 속성을 선택한 배경에는 어떠한 전문적인 지식도 고려되지 않은 것입니다. 25가지의 속성을 모두 사용해볼 수도 있겠죠. 어떤 속성들을 활용할지는 설계자의 몫입니다. 그런데 여러분이 기상과 관련된 전문적인 지식을 갖고 있지 않는 이상 이중에서 어떤 속성이 기온 예측에 가장 중요한지, 또는 어떤 속성이 가장 불필요한 속성인지 알지 못할 것입니다. 하지만 고맙게도 딥러닝은 이러한 속성 선택 문제에 덜 예민한 학습 방식입니다. 더 정확하게 말하면, 다소 불필요한 정보가 입력으로 주어진다고 해서 극단적으로 학습이 이루어지지 않는 일은 일어나지 않을 가능성이 큽니다. 학습과정에서 인공신경망이 필요한 특징(feature)을 알아서 추출하기 때문입니다. 그러니까 우리는 어떤 속성을 활용할지를 너무 심각하게 고민하지 않아도 되는 것입니다. 다만 이번 실습에서는 매번 빠짐없이 잘 기록된 속성들을 위주로 9가지를 선택한 것 뿐입니다.\n",
    "\n",
    "아래에 정의한 전처리 `preprocess` 함수는 csv파일들을 읽어 9가지 속성 정보만을 NumPy 배열에 저장해 반환하는 역할을 합니다. 이 함수는 이후에 `train_data`, `val_data`, `test_data`를 구현할 때 활용될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyIzJjHJbe3m"
   },
   "outputs": [],
   "source": [
    "def preprocess(all_files):\n",
    "    data_0 = []  # 기온\n",
    "    data_1 = []  # 강수량\n",
    "    data_2 = []  # 풍속\n",
    "    data_3 = []  # 습도\n",
    "    data_4 = []  # 증기압\n",
    "    data_5 = []  # 이슬점 온도\n",
    "    data_6 = []  # 현지 기압\n",
    "    data_7 = []  # 해면 기압\n",
    "    data_8 = []  # 지면 온도\n",
    "    for f in all_files:\n",
    "        with open(f, encoding='euc-kr') as c:\n",
    "            csv_reader = csv.reader(c, delimiter=',')\n",
    "            header = True\n",
    "            for col in csv_reader:\n",
    "                if header:\n",
    "                    header = False\n",
    "                    continue\n",
    "                data_0.append(\n",
    "                    float(col[2])) if col[2] != '' else data_0.append(0.0)\n",
    "                data_1.append(\n",
    "                    float(col[3])) if col[3] != '' else data_1.append(0.0)\n",
    "                data_2.append(\n",
    "                    float(col[4])) if col[4] != '' else data_2.append(0.0)\n",
    "                data_3.append(\n",
    "                    float(col[6])) if col[6] != '' else data_3.append(0.0)\n",
    "                data_4.append(\n",
    "                    float(col[7])) if col[7] != '' else data_4.append(0.0)\n",
    "                data_5.append(\n",
    "                    float(col[8])) if col[8] != '' else data_5.append(0.0)\n",
    "                data_6.append(\n",
    "                    float(col[9])) if col[9] != '' else data_6.append(0.0)\n",
    "                data_7.append(\n",
    "                    float(col[10])) if col[10] != '' else data_7.append(0.0)\n",
    "                data_8.append(\n",
    "                    float(col[22])) if col[22] != '' else data_8.append(0.0)\n",
    "\n",
    "    data = np.zeros((len(data_0), 9))\n",
    "    for i, d in enumerate(data):\n",
    "        data[i, 0] = data_0[i]\n",
    "        data[i, 1] = data_1[i]\n",
    "        data[i, 2] = data_2[i]\n",
    "        data[i, 3] = data_3[i]\n",
    "        data[i, 4] = data_4[i]\n",
    "        data[i, 5] = data_5[i]\n",
    "        data[i, 6] = data_6[i]\n",
    "        data[i, 7] = data_7[i]\n",
    "        data[i, 8] = data_8[i]\n",
    "\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye_O4_oVbe3o"
   },
   "source": [
    "**preprocess**를 해주고 나면, 데이터가 다음 그림과 같이 매 시간마다 총 9가지의 속성만 저장됩니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1iA_SHcfJ9XnZ2w_S66GS_XpvRH7WNXGf\" width=\"600px\" height=\"400px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m77dVMjDbe3p"
   },
   "outputs": [],
   "source": [
    "data_dir = './data/climate_seoul'\n",
    "\n",
    "train_data = preprocess(sorted(glob.glob(os.path.join(data_dir, 'train', '*'))))\n",
    "val_data = preprocess(sorted(glob.glob(os.path.join(data_dir, 'val', '*'))))\n",
    "test_data = preprocess(sorted(glob.glob(os.path.join(data_dir, 'test', '*'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6970,
     "status": "ok",
     "timestamp": 1583156254654,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "vMe2O-Z1be3s",
    "outputId": "aaf4d78e-fb83-46ca-caad-f4908f287cc1"
   },
   "outputs": [],
   "source": [
    "print(\"shape of train data: {}\".format(train_data.shape))\n",
    "print(\"shape of val data: {}\".format(val_data.shape))\n",
    "print(\"shape of test data: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKm4Y_iFbe3v"
   },
   "source": [
    "## 4. 데이터 샘플 시각화\n",
    "\n",
    "`train_data`의 첫번째 `column` 즉 온도 데이터를 그래프 형태로 시각화합니다.\n",
    "\n",
    "$\\Delta t = 240$: 240시간, 즉 10일 동안의 온도 그래프를 그려봅니다. 그림을 보면 10개의 주기적으로 나타나는 뽀죡한 고 점들을 볼 수 있는데 이 점들이 하루의 낮 최고 기온을 의미하는 것으로 보입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7166,
     "status": "ok",
     "timestamp": 1583156254854,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "5uugQmkybe3w",
    "outputId": "0cc1b9ac-611a-43a3-ee19-bfae5aac4c74"
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "dt = 240\n",
    "plt.plot(train_data[start:start+dt, 0])\n",
    "plt.ylabel(\"temperature\")\n",
    "plt.xlabel(\"time-delta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kdsYfPqbe3z"
   },
   "source": [
    "## 5. Dataset 만들기\n",
    "\n",
    "이제 우리가 사용할 데이터셋에 대해 정의할 차례입니다. 이를 위해  `make_dataset` 함수를 만들 것이며 여기서 사용되는 각 인수의 의미는 다음과 같습니다.\n",
    "\n",
    "- `data`: **<3. 데이터 전처리 함수 정의>** 에서 정의한 전처리 `preprocess` 함수로 얻은 `train_data`, `val_data`, `test_data` 가 됩니다. `data`의 shape은 (데이터의 총 길이, 9)가 됩니다.\n",
    "- `seq_length`: 우리가 이후에 설계할 RNN의 입력으로 줄 데이터 시퀀스의 길이, 즉 총 타임스텝의 길이를 뜻합니다. 다시 말하면, 현재로부터 24시간 뒤의 기온을 예측하기 위해 얼마만큼의 과거 정보를 참고할지 결정하는 것입니다. 기본값을 480로 해두었는데, 이는 480개의 과거 데이터를 살펴보겠다는 것이고, 데이터 1개는 1시간마다 기록되기 때문에 결과적으로 지난 20일간의 데이터를 기반으로 24시간 후의 기온을 예측하겠다는 의미입니다.\n",
    "- `target_delay`: 우리가 예측할 시점이 입력 시퀀스의 마지막 타임스텝으로 얼만큼 이후인지 결정하는 것입니다. 24로 기본값을 해두었고, 이는 24시간 이후의 기온이 우리가 예측할 대상임을 의미하는 것입니다. 예를 들어 1월 1일 00:00 부터 1월 21 일 23:00 까지 480 개의 데이터를 이용한다면 예측 데이터는 1월 21일 23:00 로부터 24 시간 후인 1월 22일 23:00 시점의 기온입니다.\n",
    "- `strides`: 학습할 데이터의 양을 조절하기 위해 만든 옵션 입니다. $\\Delta t = 10$, `strides=1`이라고 할 때 $\\mathbf{x}^{(1)} = [x_{1}, x_{2}, \\cdots, x_{10}]$, $\\mathbf{x}^{(2)} = [x_{2}, x_{3}, \\cdots, x_{11}]$, $\\mathbf{x}^{(3)} = [x_{3}, x_{4}, \\cdots, x_{12}]$ 이런식으로 데이터를 사용합니다. 우리가 사용하는 데이터의 time step이 많기 때문에 `strides=1`이면 굉장히 많은 수의 training data를 가질 수 있습니다. gpu가 없거나 빠른 테스트를 위한다면 `strides`를 조절하여 데이터 양을 조절하면 좋을 것 같습니다. 예를들어 `strides=3`이면 $\\mathbf{x}^{(1)} = [x_{1}, x_{2}, \\cdots, x_{10}]$, $\\mathbf{x}^{(2)} = [x_{4}, x_{5}, \\cdots, x_{13}]$, $\\mathbf{x}^{(3)} = [x_{7}, x_{8}, \\cdots, x_{16}]$ 이와 같이 sampling을 하여 데이터를 사용합니다.\n",
    "<img src=\"https://drive.google.com/uc?id=1DJ6lybrsRD8D8TtHBvbo-DU7zi-U4JMm\"  width=\"800px\" height=\"300px\" />\n",
    "- `mode`: `train`, `val`, `test`를 뜻합니다.\n",
    "- `train_mean`, `train_std`: 입력으로 사용하는 데이터의 각 속성은 저마다 값의 범위가 다릅니다. 예를 들어 기온은 보통 -15에서 35사이의 값을 갖지만, 강수량과 풍속은 음수 값이 존재하지 않고, 기압의 경우에는 1000 내외의 값이 일반적입니다. 이러한 경우 각각의 속성들을 저마다의 평균과 표준편차를 통해 값을 정규화(normalize)해주는 것이 바람직합니다. 정규화 시 `train`, `val`, `test` 각각의 데이터셋에 대해서 진행하는 것이 아니라 `train` 데이터 셋에서 구한 평균인 `train_mean`과 표준편차인 `train_std`를 이용하여 `val`, `test` 데이터 셋을 정규화해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rS4ir6u8be3z"
   },
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "이제 다음을 읽고 코드를 완성해보세요.\n",
    "1. `mode`에 맞게 정규화(normalize) 코드를 작성해보세요.\n",
    "    - 1-1 : `train` 모드일 때: `train` 디렉토리에 있는 데이터 셋의 `mean`, `std`를 구해야 합니다. 함수를 사용할 경우 `axis`를 주의해 주세요.\n",
    "    - 1-2 : `val` 또는 `test` 모드일 때: `train_mean`, `train_std`를 이용합니다.\n",
    "    - 1-3 : normalize 코드를 작성해보세요. (수식은 다음과 같습니다.)\n",
    "        - $ (X - \\mathbb{E}(X)) \\, / \\, \\sqrt{ \\mathrm{Var} (X) } $.\n",
    "        - $\\mathbb{E}(X)$ 는 데이터의 평균, $\\sqrt{ \\mathrm{Var} (X) }$ 는 데이터의 표준편차입니다.\n",
    "2. `for`문으로 전체 타임스텝 (`range(len(data) - seq_length - target_delay)`)을 돌면서 인덱스(변수명 `index`)가 `stride`로 나누어 떨어질 때만 훈련 데이터 셋으로 저장을 합니다.\n",
    "    - 2-1 : `stride=3` 이면 `index`가 0, 3, 6, 9, ... 일 때만 `sequence`와 `target`에 데이터를 집어넣는 것입니다. 즉 해당 인덱스는 각 배치 데이터의 첫번째 인덱스가 됩니다. 따라서 `data` 변수에 해당하는 데이터를 `index`부터 `seq_length`(480시간)만큼 잘라내어 `sequence`에 추가합니다. 이는 곧 모델에게 전달되는 입력 데이터가 됩니다.\n",
    "    - 2-2 : `data`의 `index`부터 `seq_length`만큼의 떨어진 위치에서 다시 `target_delay`(즉 24시간) 만큼 떨어진 위치의 기온(0번째 컬럼)을 target에 추가합니다.\n",
    "3. 최종 Numpy로 변경되었을때 `sequence`의 shape은 (N, `seq_length`, 9), `target`의 shape은 (N,)이 되어야 합니다. N은 전체 데이터 갯수 입니다.\n",
    "\n",
    "**make_dataset 함수 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhzNK5D8be30"
   },
   "outputs": [],
   "source": [
    "def make_dataset(data, seq_length=480, target_delay=24, strides=5,\n",
    "                 mode='train', train_mean=None, train_std=None):\n",
    "    '''\n",
    "        각 인수에 대한 설명은 위의 'Dataset만들기' 설명을 참고하세요.\n",
    "    '''\n",
    "    assert mode in ['train', 'val', 'test']\n",
    "    if mode is not 'train':\n",
    "        if train_mean is None or train_std is None:\n",
    "            print('Current mode is {}'.format(mode))\n",
    "            print('This mode needs mean and std of train data')\n",
    "            assert False\n",
    "\n",
    "    # 정규화\n",
    "    ## 코드 시작 ##\n",
    "    if mode is 'train':\n",
    "        mean = None    # 위의 설명 1-1 을 참고하여 None을 채우세요.\n",
    "        std = None     # 위의 설명 1-1 을 참고하여 None을 채우세요.\n",
    "    else: \n",
    "        mean = None    # 위의 설명 1-2 를 참고하여 None을 채우세요.\n",
    "        std = None     # 위의 설명 1-2 를 참고하여 None을 채우세요.\n",
    "    data = None        # 위의 설명 1-3 을 참고하여 None을 채우세요.\n",
    "    ## 코드 종료 ##\n",
    "    \n",
    "    # 입력, 타겟 데이터 생성\n",
    "    sequence = []\n",
    "    target = []\n",
    "    for index in range(len(data) - seq_length - target_delay):\n",
    "        if index % strides == 0:\n",
    "            \n",
    "            ## 코드 시작 ##\n",
    "            sequence.append(None)    # 위의 설명 2-1 을 참고하여 None을 채우세요.\n",
    "            target.append(None)      # 위의 설명 2-2 를 참고하여 None을 채우세요.\n",
    "            ## 코드 종료 ##\n",
    "\n",
    "    if mode is 'train':\n",
    "        return np.array(sequence), np.array(target), mean, std\n",
    "    else:\n",
    "        return np.array(sequence), np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xIt7DgWgbe32"
   },
   "source": [
    "이제 학습용, 검증용, 테스트용 dataset을 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdkIOAjtbe33"
   },
   "outputs": [],
   "source": [
    "train_sequences, train_labels, train_mean, train_std = make_dataset(train_data, mode='train')\n",
    "val_sequences, val_labels = make_dataset(val_data, mode='val', train_mean=train_mean, train_std=train_std)\n",
    "test_sequences, test_labels = make_dataset(test_data, mode='test', train_mean=train_mean, train_std=train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeTEKisKbe36"
   },
   "source": [
    "#### Input pipeline 만들기\n",
    "\n",
    "`tf.data.Dataset`을 이용하여 `train`, `val`, `test`에 대한 `dataset`을 만들어 봅니다.\n",
    "* `tf.data.Dataset.from_tensor_slices`: `numpy`타입(또는 리스트 형태의 데이터)의 데이터를 `tf.data.Dataset`포맷의 데이터로 변환시켜줍니다.\n",
    "* `tf.data.Dataset.shuffle`: 데이터셋을 shuffle 시켜줍니다.\n",
    "* `tf.data.Dataset.batch`: 데이터셋의 batch_size를 결정합니다.\n",
    "* `tf.data.Dataset.repeat`: 데이터셋을 다 소진하더라도 다시 사용할 수 있게합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfbtJqZEbe37"
   },
   "outputs": [],
   "source": [
    "# shuffle의 인자로 buffer_size가 필요한데 이는 전체 데이터셋 갯수로 하는게 좋습니다.\n",
    "N = BUFFER_SIZE = len(train_sequences) # number of samples in train_dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z96Lk92Ube39"
   },
   "source": [
    "`val`, `test`데이터 셋은 shuffle할 필요가 없습니다. \n",
    "* `train`시 shuffle하는 목적은 mini-batch gradient descent를 하기 위해 mini-batch 데이터를 random 하게 뽑는 것입니다. \n",
    "* `val`, `test` 데이터 셋의 목적은 모든 데이터셋을 다 보고 성능을 평가하기 위함입니다. 그렇기 때문에 `val`, `test`데이터 셋은 shuffle할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dY5Ua-Hbbe3-"
   },
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sequences, val_labels))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMd5X4JEbe4B"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9jKmNLkbe4D"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7390,
     "status": "ok",
     "timestamp": 1583156255104,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "DZPwlIiube4E",
    "outputId": "81113c8a-ede6-473e-db2a-c9c61d048ef7"
   },
   "outputs": [],
   "source": [
    "checker.customized_dataset_check(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfvHJ-Jebe4G"
   },
   "source": [
    "## 6. 베이스라인 성능 측정\n",
    "\n",
    "이후에 우리가 학습시킨 모델의 성능의 좋고 나쁨을 판단할 기준(베이스라인)이 필요합니다. 적당한 수준의 기준 성능을 정해놓고 테스트 결과에서 우리의 모델이 그것보다 더 좋은 성능을 보이면 만족할만한 모델을 학습시킨 것으로 간주하면 되고, 반대로 그보다 성능이 좋지 않다면 모델을 더 개선시키는 방식으로 네트워크 구조를 변경해 나아가면 됩니다.\n",
    "\n",
    "과거의 기후 정보를 활용하는 우리의 딥러닝 모델이 과연 의미있는 성능이라는 걸 보이기 위해서는 어떠한 예측 방식이 기준이 될 수 있을까요? 아마 기온 예측 전문가가 아닌 대부분의 여러분이 지금으로부터 24시간 후의 기온을 예측하라는 질문을 받았다고 생각해 봅시다. 아마 가장 무난하면서도 안정적으로 예측하는 방식은 지금의 기온과 같다고 답하는 것일 겁니다. 이러한 예측 방식은 과거의 기후 정보를 복잡하게 고려할 필요도 없이 간단하지만, 많은 경우에 실제로 꽤나 정확한 예측을 할 수 있는 방식입니다. 내일 낮 12시의 기온은 특이한 경우가 아니라면, 오늘 낮 12시의 기온과 거의 비슷할 것입니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "아래의 `eval_baseline` 함수는 베이스라인 모델의 평균 loss를 측정하는 함수입니다. 다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "1. 우리의 베이스라인은 현재 기온을 24시간 후의 기온으로 예측하는 방식입니다. 현재 기후 정보는 `test_dataset`에서 받은 `sequences`의 가장 마지막 타임스텝에 담겨있습니다. 입력으로 주는 `sequences`는 기온 뿐만 아니라 총 9가지 속성이 포함된 것임에 유의하세요. 기온은 9가지 속성중 가장 첫번째에 위치하고 있음을 염두에 두고 `sequences`로 부터 현재 \"기온\"을 가져와 `predictions` 변수에 저장하세요.\n",
    "2. `loss_fn` 은 차후에 넣을 손실함수(loss function)입니다. `loss_fn` 함수의 첫번째 인자로 추정치인 `predictions`, 두번째 인자로 실제 24시간 이후의 기온인 `targets`을 이용하여 측정된 loss를 `loss` 변수에 저장하세요. (힌트: `loss_fn`는 함수 타입을 넣을 것임으로 다음과 같이 사용할 수 있습니다. <br> `함수(1번 인자, 2번인자, ...)`)\n",
    "\n",
    "**eval_baseline 함수 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBWmiABSbe4H"
   },
   "outputs": [],
   "source": [
    "def eval_baseline(dataset, loss_fn):\n",
    "    mean_loss = tf.keras.metrics.Mean()\n",
    "    for sequences, targets in dataset:\n",
    "        ## 코드 시작 ##\n",
    "        predictions = None    # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
    "        loss = None           # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "        ## 코드 종료 ##\n",
    "        \n",
    "        mean_loss(loss)\n",
    "        \n",
    "    print('Baseline Average Loss: {:.4f}'.format(mean_loss.result()))\n",
    "    return mean_loss.result()\n",
    "\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "baseline_loss = eval_baseline(test_dataset, loss_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjkZsy38be4J"
   },
   "source": [
    "베이스라인의 성능을 측정합니다. 다음과 같은 결과가 출력된다면 성공적으로 구현한 것입니다.\n",
    "\n",
    "> Baseline Average Loss: 0.1058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8267,
     "status": "ok",
     "timestamp": 1583156255988,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "I7jGMs1Xbe4K",
    "outputId": "729ebace-1153-4625-cc1c-2b5eb6cd632d"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "baseline_loss = eval_baseline(test_dataset, loss_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7D3yMEagbe4N"
   },
   "source": [
    "평균 loss만 봐서는 베이스라인이 어느 정도로 예측을 잘하는지 감이 잘 오지 않습니다. 베이스라인 모델의 예측 기온과 실제 기온을 몇가지 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8263,
     "status": "ok",
     "timestamp": 1583156255989,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "Y6vSXg7wbe4N",
    "outputId": "c1330f70-fa4e-4104-be22-2bdf30a00f0b"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data_idx = np.random.randint(len(test_sequences))\n",
    "    pred = test_sequences[data_idx, -1, 0]\n",
    "    # 예측 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    pred = pred * train_std[0] + train_mean[0]\n",
    "    # 실제 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    target = test_labels[data_idx] * train_std[0] + train_mean[0]\n",
    "    print('예측 기온: {:.1f} / 실제 기온: {:.1f} / 차이: {:.1f}'.format(\n",
    "        pred, target, pred - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOXCI8Gabe4P"
   },
   "source": [
    "## 7. 네트워크 설계\n",
    "\n",
    "우리는 LSTM 구조를 통해 기온 예측을 모델을 학습시킬 것입니다. 설계할 네트워크의 대략적인 개요는 아래 그림과 같습니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1AjxdDivdX23DSdubayKhAAcXmWfRBSvb\"  width=\"800px\" height=\"400px\" />\n",
    "\n",
    "LSTM의 매 타임스텝의 입력은 매 시간마다 기록된 9가지 속성값이 들어가게 됩니다. 그리고 마지막 타입스텝의 출력을 마무리로 Dense 레이어에 넣어 최종 예측 기온값을 출력하는 구조입니다.  \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "우리가 구현할 클래스는 `SimpleLSTM`입니다. 다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "1. [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)을 사용하여 모델을 만들어보세요.\n",
    "    - `tf.test.is_gpu_available()`를 사용하시면 GPU 사용이 가능한지 알 수 있습니다.\n",
    "    - 여러분이 GPU를 가지고 계시다면 Document에서 CuDNN 부분을 확인해주세요.\n",
    "2. 생성자의 파라미터 `hidden_sizes`는 우리가 설계할 LSTM 레이어의 `hidden_sizes`의 크기입니다. 이 크기를 얼마를 할지는 역시 설계자의 몫입니다. `hidden_sizes`는 리스트로 값을 받습니다. 예를 들어 `hidden_sizes = [4, 8, 16]` 이면 첫 번째 layer의 `units=4`, 두 번째 layer의 `units=8`, 세 번째 layer의 `units=16` 입니다.\n",
    "    -  **<2. 하이퍼파라미터 세팅>** 에서 우리는 첫 번째 layer의 `units=100`, 두 번째 layer의 `units=100`으로 하였습니다.\n",
    "3. 여러 층으로 쌓고 싶다면 쌓을 `layer`들에는 꼭 `return_sequences=True` 옵션을 주어야 합니다.\n",
    "4. 맨 마지막 층의 `rnn` 레이어의 최종 state를 가지고 `dense` 연산을 할 것 입니다. 따라서 맨 마지막은 `outputs` 값으로 모든 sequence 정보가 필요 없습니다. `return_sequences=False` (default) 를 하시면 됩니다.\n",
    "5. `self.rnn`에서 나온 최종 state를 `dense` 레이어에 연결합니다. 이때 [`layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)를 이용하면 됩니다. 우리는 \"기온\"이라는 하나의 실수를 예측할 것을 기억하세요!\n",
    "\n",
    "**모델 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5q-rXqnbe4V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_sizes, output_dim=1, num_layers=1, batch_size=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_sizes[0],\n",
    "                            num_layers=self.num_layers, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_sizes[-1], self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_sizes[0])\n",
    "        c0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_sizes[0])\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        y_pred = self.linear(out[:, -1, :])\n",
    "        return y_pred.view(-1)\n",
    "\n",
    "model = SimpleLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nrjpc_dVbe4W"
   },
   "source": [
    "아래의 코드 블록은 데이터를 집어 넣어서 잘 동작하는지 확인해 보는 코드 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQ1lYqpgbe4X"
   },
   "outputs": [],
   "source": [
    "for step, (sequences, targets) in enumerate(train_dataset.take(1)):\n",
    "    predictions = model(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8nyzgMbe4c"
   },
   "source": [
    "`summary()` API로 모델이 잘 구성됐는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14253,
     "status": "ok",
     "timestamp": 1583156261997,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "rcpk20Clbe4d",
    "outputId": "ba407391-8690-4f28-c144-4ac7d99f8ce6"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqinVZSVbe4f"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14252,
     "status": "ok",
     "timestamp": 1583156261998,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "N_POevr8be4g",
    "outputId": "86ac422a-b81d-48f8-ae1a-3b63897203b4"
   },
   "outputs": [],
   "source": [
    "checker.model_check(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "so9IvuHxbe4i"
   },
   "source": [
    "## 8. Loss function, Optimizer 정의\n",
    "\n",
    "생성한 모델을 학습 시키기 위해서 손실함수를 정의해야 합니다. 뉴럴네트워크는 경사하강(gradient descent)방법을 이용하여 손실함수의 값을 줄이는 방향으로 파라미터를 갱신(update) 하게 됩니다. 또한 효과적인 경사하강 방법을 적용하기 위해 옵티마이져를 함께 사용할 겁니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. \n",
    "\n",
    "* `compile` 앞서 정의한 model에서 training을 위해 loss와 optimizer를 지정해주세요. [`tf.Keras.Model.compile()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)\n",
    "* 연속된 값을 예측하는 Regression 문제에서는 [`Mean Squared Error`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)를 사용합니다. MSE를 `loss`로 지정해주세요.\n",
    "* 이번 실습에서는 [`Adam optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?version=stable)를 사용하겠습니다. - `metrics` Regression 문제에서는 MSE가 metrics이기도 합니다.\n",
    "\n",
    "### <font color='blue'> [Tensorflow 1.x 버전과 2.x 버전의 차이점] </font>\n",
    "\n",
    "- `loss`, `optimizer`, `metric` 등을 `model.compile`을 통해 한꺼번에 쉽고 가독성 있게 설정할 수 있게 되었습니다. \n",
    "\n",
    "**이제 손실함수와 옵티마이저 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlOQDIEzbe4k"
   },
   "outputs": [],
   "source": [
    "## 코드 시작 ##\n",
    "model.compile(loss=None, \n",
    "              optimizer=None, \n",
    "              metrics=[None])\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkIeQl1Lbe4m"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14248,
     "status": "ok",
     "timestamp": 1583156262000,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "rgEVFj7zbe4n",
    "outputId": "3fcd8065-b2b6-45d7-f682-80b9a04380f1"
   },
   "outputs": [],
   "source": [
    "checker.compile_check(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWczPmlfbe4p"
   },
   "source": [
    "## 모델 저장하기\n",
    "\n",
    "`callback`을 이용하면 모델이 학습하면서 얻은 weight를 저장할 수 있습니다. 매 epoch마다 weights만 저장하는 코드를 작성해보세요.\n",
    "\n",
    "저장한 모델 파일을 `model.load_weights()`를 통하여 불러올 수 있습니다. 모델의 저장 및 불러오기를 좀 더 자세히 알고 싶으면 [Save and restore](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko) 문서를 참고하세요.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. \n",
    "\n",
    "* 아래의 셀에서 채워야 할 인자는 `checkpoint_path`, `save_weights_only`, `save_freq` 입니다.\n",
    "* verbose=1은 구현에 영향을 미치지는 않습니다. 정보 표현의 정도를 나타내는 것인데, 예를들면 verbose가 1이면 '이름'만을 말하고 2이면 '이름', '성별' 3이면 '이름', '성별', '나이'를 말하는 것입니다.\n",
    "\n",
    "**이제 callback코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjThnywobe4p"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./train/exp_rnn/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# 체크포인트 콜백 만들기\n",
    "## 코드 시작 ##\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(None, verbose=1)\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ktu8gd7be4s"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14242,
     "status": "ok",
     "timestamp": 1583156262000,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "kb22NleTbe4s",
    "outputId": "5b6c222b-2c6d-4561-b469-f075030d8a04"
   },
   "outputs": [],
   "source": [
    "checker.callback_check(cp_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6Aq0fjdbe4v"
   },
   "source": [
    "## 8. Training\n",
    "\n",
    "LSTM과 같은 recurrent layer는 타임스텝을 따라 계산이 순차적으로 진행을 해야 되기 때문에 상대적으로 학습 시간이 많이 소요됩니다. 따라서 시간이 여유가 없는 분들은 모델 학습이 적당히 진행된다는 정도만 확인하고 다음 단계로 넘어가셔도 됩니다.\n",
    "\n",
    "만약 한 에폭 이상이 지나도록 loss가 전혀 감소하는 기미가 보이지 않는다면 이전의 구현에 문제가 있을 가능성이 높으니 코드를 다시 검토해주시기 바랍니다. 이러한 경우에는 구현한 `fit` 함수를 다시 한 번 확인하시기 바랍니다. \n",
    "\n",
    "또한, 모델 저장 코드를 제대로 구현했다면 첫 에폭 학습후에 `checkpoint_dir` 경로에 `cp-epoch.ckpt` 파일이 저장되어 있어야 합니다. 만약에 파일이 존재하지 않는다면 모델 저장 코드를 다시 확인하시기 바랍니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "- [`tf.keras.Model의 Method인 fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)을 이용하여 학습을 시작하세요.\n",
    "- `tf.data.Dataset`으로 만든 객체인 `train_dataset`을 input으로 넣어주세요.\n",
    "- input으로 tf.data를 이용하는 경우 `steps_per_epoch` 을 지정해주어야 합니다. train_data의 길이를 batch_size 크기로 나눈 값을 지정해주세요.\n",
    "- `validation_data`와 `validation_steps` 도 같이 지정해주세요.\n",
    "- `epochs`를 미리 변수에 담아둔 값으로 지정해주세요.\n",
    "- `callbacks`를 이용하여 모델 weight를 저장해주세요.\n",
    "\n",
    "**이제 각 스텝에 따라 훈련 단계 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 245822,
     "status": "ok",
     "timestamp": 1583156493584,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "TAAnXT8rbe4w",
    "outputId": "479324e9-5a8b-428e-98fa-f8d90be8e894"
   },
   "outputs": [],
   "source": [
    "## 코드 시작 ##\n",
    "model.fit(train_dataset, steps_per_epoch = None, \n",
    "          validation_data = None, \n",
    "          validation_steps= None,\n",
    "          epochs= None,\n",
    "          callbacks= [None])\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xz2dz15be4y"
   },
   "source": [
    "## 9. 저장된 모델 불러오기 및 test\n",
    "\n",
    "학습한 모델의 성능을 테스트합니다. 저장한 모델 파일을 `model.load_weights()`를 통하여 불러올 수 있습니다. 모델의 저장 및 불러오기를 좀 더 자세히 알고 싶으면 [Save and restore](https://www.tensorflow.org/guide/keras#save_and_restore) 문서를 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "`7. 네트워크 설계`와 `8. Loss function, Optimizer 정의` 에서 코드 구현을 문제 없이 하셨다면 그대로 적으시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gf4LV0YVbe4z"
   },
   "outputs": [],
   "source": [
    "# 아래의 모델 불러오기를 정확히 구현했는지 확인하기 위해 새로 모델을 선언하여 학습 이전 상태로 초기화\n",
    "model = tf.keras.Sequential()\n",
    "num_layers = len(hidden_sizes)\n",
    "\n",
    "for i in range(num_layers - 1):\n",
    "    ## 코드 시작 ##\n",
    "    model.add(None)\n",
    "    model.add(None)\n",
    "    model.add(None)\n",
    "    model.compile(None)\n",
    "    ## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 246001,
     "status": "ok",
     "timestamp": 1583156493771,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "jMgkgyFkbe40",
    "outputId": "c1d3dd0d-d714-41bc-894b-16d5552b2e33"
   },
   "outputs": [],
   "source": [
    "# inputs을 넣어 모델을 생성\n",
    "for sequences, targets in train_dataset.take(1):\n",
    "    outputs = model(sequences)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 246186,
     "status": "ok",
     "timestamp": 1583156493960,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "UMjRCun7be42",
    "outputId": "5b9e2161-b0f9-4818-e827-a33ffeb0daa9"
   },
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JHNrEB0be44"
   },
   "source": [
    "마지막으로 모델의 성능을 테스트합니다. 베이스라인 성능을 뛰어 넘었다는 문구가 나오면 성공적으로 진행한 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 247821,
     "status": "ok",
     "timestamp": 1583156495599,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "cv5bgsl_be45",
    "outputId": "e42b49eb-b2f3-40dd-bff1-b34af1960267"
   },
   "outputs": [],
   "source": [
    "_, test_loss = model.evaluate(test_dataset, steps=len(test_sequences)/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* '_'는 변수를 지정을 해야하는데 쓰지 않을 때 사용합니다. 아래의 checker.performance_check()에서 test_loss만 확인하므로 지정하지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "In6ivsrAbe4-"
   },
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 247818,
     "status": "ok",
     "timestamp": 1583156495600,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "PHqOPLCTbe4_",
    "outputId": "70c74045-0ffd-40d4-8eb0-a6eed5487e67"
   },
   "outputs": [],
   "source": [
    "checker.performance_check(test_loss, baseline_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mH6EYk75be5B"
   },
   "source": [
    "학습된 모델의 예측 기온과 실제 기온을 몇가지 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 248002,
     "status": "ok",
     "timestamp": 1583156495787,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "vw4LJ154be5B",
    "outputId": "d939a75e-8352-44d5-f06b-0d7cabec4560"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data_idx = np.random.randint(len(test_sequences))\n",
    "    pred = model(test_sequences[data_idx:data_idx+1, :, :]).numpy()[0, 0]\n",
    "    # 예측 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    pred = pred * train_std[0] + train_mean[0]\n",
    "    # 실제 기온을 normalization 이전 상태(섭씨 단위)로 되돌리는 작업\n",
    "    target = test_labels[data_idx] * train_std[0] + train_mean[0]\n",
    "    print('예측 기온: {:.1f} / 실제 기온: {:.1f} / 차이: {:.1f}'.format(\n",
    "        pred, target, pred - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDkLoPK6be5E"
   },
   "source": [
    "어떠신가요? 잘 맞는것 같나요? 모델 구조의 은닉층 크기(`hidden_size`) 등 다양한 하이퍼파라미터를 조절해서 더 좋은 성능을 내는 모델을 만들어보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRADTxbXbe5G"
   },
   "source": [
    "## 12. Summary\n",
    "\n",
    "이로써 마지막 RNN 프로젝트를 완료했습니다. 고생하셨습니다! \n",
    "\n",
    "우리는 이번 실습을 통해 다음과 같은 내용을 학습했습니다.\n",
    "- 날씨와 같은 시계열 정보를 다룰 수 있다.\n",
    "- RNN을 설계하고 시간 순서상 미래의 정보를 예측하는 모델을 학습시킬 수 있다.\n",
    "- 상식적인 수준의 베이스라인을 도입하여 학습한 모델의 성능을 검증할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_JbIt4Nbe5G"
   },
   "source": [
    "# Self-Review\n",
    "\n",
    "학습 환경에 맞춰 알맞는 제출방법을 실행하세요!\n",
    "\n",
    "### 로컬 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 혹은 `File > Save and checkpoint`로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 주의할 점은 Jupyter Notebook 의 파일이름을 수정하시면 안됩니다! 만약에 노트북 이름을 수정했다면 \"tensorflow-rnn-project\" 로 바꿔주시길 바랍니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"submit.zip\"이 생깁니다. \"cnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"rnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "    * \"rnn_submission.html\" : 여러분이 작성한 Jupyter Notebook 을 html 형식으로 전환한 파일\n",
    "3. 코드 실행결과 안내에 따라서 `submit.zip` 파일을 확인하시고 제출해주시길 바랍니다.\n",
    "\n",
    "### Colab 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 코드 실행결과 안내에 따라서 재작성하거나 다음스텝으로 넘어갑니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"rnn_submission.tsv\"만 생깁니다. \"rnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"rnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "3. 프로젝트를 저장한 드라이브의 `submit` 폴더에서 `rnn_submission.tsv` 파일을 다운 받습니다.\n",
    "4. Colab Notebook 에서 `파일 > .ipynb 다운로드` 를 통해서 노트북을 다운로드 받습니다.\n",
    "5. 로컬에서 Jupyter Notebook 프로그램을 실행시킵니다. \n",
    "6. 4번 스텝에서 다운받은 노트북을 열고 `File > Download as > HTML(.html)` 로 재 다운로드 합니다.\n",
    "7. 3번 스텝에서 받은 파일과 6번 스텝에서 받은 파일을 하나의 폴더에 넣고, `submit.zip` 이라는 이름으로 압축하고 제출해주시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 248562,
     "status": "ok",
     "timestamp": 1583156496349,
     "user": {
      "displayName": "염치민폐",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1ig-ebw8d9YYrmJ18LDLVJzZX7P03OcZbU56S=s64",
      "userId": "00574487206534282477"
     },
     "user_tz": -540
    },
    "id": "9LQfmtoXbe5H",
    "outputId": "4d586c0c-697f-4594-c782-76a302ae6677"
   },
   "outputs": [],
   "source": [
    "import check_util.submit as submit\n",
    "submit.process_submit()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "(ans)tensorflow2.0-rnn-project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
