{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## project Nuclear Plant status\n",
    "\n",
    "data source : \n",
    "https://dacon.io/competitions/open/235551/data\n",
    "https://www.kaggle.com/competitions/2021-ai-w6-p1/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27671\n",
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor row in range(len(data_y_original)):\\n  print(data_y[row].index(max(data_y[row])))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "#numpy.set_printoptions(threshold=numpy.Inf,linewidth=numpy.Inf) #to print the numpy list set the threshold\n",
    "data_train = numpy.genfromtxt(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0006_nn_NPP/pca_train.csv\",delimiter=\",\",encoding='UTF-8')\n",
    "data_train = data_train[1:,:]\n",
    "#print(data_train)\n",
    "\n",
    "data_y = numpy.genfromtxt(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0006_nn_NPP/train_label.csv\",delimiter=\",\",encoding='UTF-8')\n",
    "\n",
    "data_y_original = data_y[1:,1:]\n",
    "\n",
    "data_y = [[0 for col in range(int(data_y_original.max())+1)] for row in range(len(data_y_original))]\n",
    "print(len(data_y))\n",
    "print(len(data_y[0]))\n",
    "#print(data_y)\n",
    "\n",
    "for row in range(len(data_y_original)):\n",
    "  data_y[row][int(data_y_original[row][0])] = 1\n",
    "  #print(int(data_y_original[row][0]))\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "for row in range(len(data_y_original)):\n",
    "  print(data_y[row].index(max(data_y[row])))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27671\n",
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor row in range(len(data_y_original)):\\n  print(data_y[row].index(max(data_y[row])))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "data_train = numpy.genfromtxt(\"/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/pca_train.csv\",delimiter=\",\",encoding='UTF-8')\n",
    "data_train = data_train[1:,:]\n",
    "#print(data_train)\n",
    "\n",
    "data_y = numpy.genfromtxt(\"/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/train_label.csv\",delimiter=\",\",encoding='UTF-8')\n",
    "\n",
    "data_y_original = data_y[1:,1:]\n",
    "\n",
    "data_y = [[0 for col in range(int(data_y_original.max())+1)] for row in range(len(data_y_original))]\n",
    "print(len(data_y))\n",
    "print(len(data_y[0]))\n",
    "#print(data_y)\n",
    "\n",
    "for row in range(len(data_y_original)):\n",
    "  data_y[row][int(data_y_original[row][0])] = 1\n",
    "  #print(int(data_y_original[row][0]))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for row in range(len(data_y_original)):\n",
    "  print(data_y[row].index(max(data_y[row])))\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple device detected\n",
      "Activating Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "##Hardwere\n",
    "import torch\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = 'cuda'\n",
    "  templist = [1,2,3]\n",
    "  templist = torch.FloatTensor(templist).to(device)\n",
    "  print(\"Cude torch working : \",end=\"\")\n",
    "  print(templist.is_cuda)\n",
    "  print(\"current device no. : \",end=\"\")\n",
    "  print(torch.cuda.current_device())\n",
    "  print(\"GPU device count : \",end=\"\")\n",
    "  print(torch.cuda.device_count())\n",
    "  print(\"GPU name : \",end=\"\")\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  print(\"device : \",device)\n",
    "if torch.backends.mps.is_available() == True:\n",
    "  print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  print(\"cant use gpu , activating cpu\")\n",
    "  device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in lr : 1E+02 cost : 0.006972\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "#lr_list = [100000,1000,100,10,5,3,2,1,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9]\n",
    "lr_list = [100]\n",
    "nb_epoch = 10000\n",
    "for LR in lr_list:\n",
    "  \n",
    "  X = torch.FloatTensor(data_train)\n",
    "  X = X.to(device)\n",
    "  Y = torch.FloatTensor(data_y)\n",
    "  Y = Y.to(device)\n",
    "\n",
    "  #print(X.shape,Y.shape)\n",
    "\n",
    "\n",
    "  layer1 = torch.nn.Linear(256,150,bias = True)\n",
    "  layer2 = torch.nn.Linear(150,150,bias = True)\n",
    "  layer3 = torch.nn.Linear(150,150,bias = True)\n",
    "  layer4 = torch.nn.Linear(150,198,bias=True)\n",
    "\n",
    "\n",
    "  sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  model = torch.nn.Sequential(layer1,sigmoid,layer2,sigmoid,layer3,sigmoid,layer4,sigmoid)\n",
    "  model = model.to(device)\n",
    "\n",
    "  optimizer = torch.optim.SGD(model.parameters(),lr = LR)\n",
    "  #optimizer = optimizer.to(device)\n",
    "  loss = torch.nn.BCELoss()\n",
    "  loss = loss.to(device)\n",
    "  \n",
    "  for epoch in range(nb_epoch):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "    cost = loss(hypothesis,Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "  print(\"in lr : %.0E cost : %f\"%(LR,cost.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb ì…€ 8\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb#X15sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb#X15sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb#X15sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m outputs \u001b[39m=\u001b[39m NeuralNetwork(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb#X15sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, Y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/Documents/Code/Projects/ai_0006_nn_NPP/project_ai_0006_nn_Nuclear.ipynb#X15sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        drop_prob = 0.3\n",
    "        self.fc1 = nn.Linear(256, 198,bias=True)\n",
    "        self.fc2 = nn.Linear(198, 198,bias=True)\n",
    "        self.fc3 = nn.Linear(198, 198,bias=True)\n",
    "\n",
    "        self.fc4 = nn.Linear(198, 198)\n",
    "        self.relu = torch.nn.Sigmoid()\n",
    "        self.dropout = torch.nn.Dropout(p=drop_prob)\n",
    "        ## initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_() \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        drop_prob = 0.2\n",
    "        self.layer1 = nn.Linear(256, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 198)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        #self.relu = nn.Sigmoid() #you can use this too \n",
    "        self.dropout = nn.Dropout(p=drop_prob) # delete nurons randomly (p is percent) for unoverfitting\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(NeuralNetwork(X).parameters(),lr =)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "epoch=2000\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(2000)):\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    outputs = NeuralNetwork(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cost = (hypothesis-Y).mean\n",
    "    with torch.no_grad():\n",
    "        val_predict=NeuralNetwork(X)\n",
    "        _, val_predicted = torch.max(val_predict, 1)\n",
    "        score=accuracy_score(val_predicted.detach().cpu(),y_val)\n",
    "    if i%100==0:\n",
    "        print(i,\" = \",score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "\n",
    "in lr : 1E+05 cost : 0.505050  \n",
    "in lr : 1E+03 cost : 0.467331  \n",
    "#### in lr : 1E+02 cost : 0.007864  \n",
    "in lr : 1E+01 cost : 0.028546  \n",
    "in lr : 5E+00 cost : 0.029202  \n",
    "in lr : 3E+00 cost : 0.029217  \n",
    "in lr : 2E+00 cost : 0.029222  \n",
    "in lr : 1E+00 cost : 0.029226  \n",
    "in lr : 1E-01 cost : 0.029344  \n",
    "in lr : 1E-02 cost : 0.040540  \n",
    "in lr : 1E-03 cost : 0.355505  \n",
    "in lr : 1E-04 cost : 0.665495  \n",
    "in lr : 1E-05 cost : 0.685912  \n",
    "in lr : 1E-06 cost : 0.691508  \n",
    "in lr : 1E-07 cost : 0.718929  \n",
    "in lr : 1E-08 cost : 0.704015  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.36666546203607 %\n",
      "avrage diffrence : 0.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prediction = []\n",
    "#print(hypothesis)\n",
    "#hypothesis = hypothesis.tolist()\n",
    "#for row in range(len(data_y_original)):\n",
    "  #print(torch.argmax(hypothesis[row]))\n",
    "for row in range(len(data_y_original)):\n",
    "  prediction.append([torch.argmax(hypothesis[row]).item()])\n",
    "\n",
    "#print(prediction)\n",
    "accuracy = prediction == data_y_original\n",
    "print(accuracy.sum()/len(accuracy)*100,\"%\")\n",
    "\n",
    "##\n",
    "\n",
    "cost = prediction - data_y_original\n",
    "print(\"avrage diffrence : %.2f\"%(cost.sum()/len(cost)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END\n",
    "\n",
    "it was hard, but easy  \n",
    "I had to learn more about NN  \n",
    "2023/01/22/0200i  \n",
    "\n",
    "Im going to sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(28*28, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2) # delete nurons randomly (p is percent) for unoverfitting\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = NeuralNetwork()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e1691d87fd020b7f66d0635a791996ba148fb3250417110260ad29109cad53d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
