{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project : Solar Panels Productivity\n",
    "#### Source\n",
    "https://www.kaggle.com/datasets/ibrahimkiziloklu/solar-radiation-dataset  \n",
    "https://www.kaggle.com/competitions/2021-ai-w6-p2/data\n",
    "\n",
    "\n",
    "Net class\n",
    " \n",
    "https://tutorials.pytorch.kr/beginner/blitz/neural_networks_tutorial.html  \n",
    "reset weight and bias : https://discuss.pytorch.org/t/reset-model-weights/19180/3  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row :  9999\n",
      "col :  8\n",
      "data Y\n",
      "row :  9999\n",
      "col :  100\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "#get data from csv\n",
    "data = numpy.genfromtxt(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0007_nn_Solar_Panles/train.csv\",delimiter=\",\",encoding='UTF-8')\n",
    "\n",
    "\n",
    "#make list of features\n",
    "data_train = data[1:,:-1]\n",
    "print(\"row : \",len(data_train))\n",
    "print(\"col : \",len(data_train[0]))\n",
    "#print(data_train)\n",
    "\n",
    "#make list of outputs\n",
    "data_y_original = data[1:,-1:]\n",
    "data_y_dim1 = data[1:,-1]\n",
    "#print(data_y)\n",
    "\n",
    "#make a 2 dim list [data num][possible output num]\n",
    "data_y = [[0 for col in range(int(data_y_original.max())+3)] for row in range(len(data_y_original))] \n",
    "print(\"data Y\\nrow : \",len(data_y))\n",
    "print(\"col : \",len(data_y[0]))\n",
    "#print(data_y)\n",
    "\n",
    "for row in range(len(data_y)):\n",
    "  data_y[row][int(data_y_original[row][0])] = 1\n",
    "  #print(int(data_y_original[row][0]))\n",
    "#I could use round (반올림) but thats too compliacted so I didnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y_original.shape\n",
    "data_y_dim1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda torch working : True\n",
      "current device no. : 0\n",
      "GPU device count : 1\n",
      "GPU name : NVIDIA GeForce GTX 1080\n",
      "device :  cuda\n"
     ]
    }
   ],
   "source": [
    "##Hardwere\n",
    "import torch\n",
    "if torch.cuda.is_available() == True:\n",
    "  device = 'cuda'\n",
    "  templist = [1,2,3]\n",
    "  templist = torch.FloatTensor(templist).to(device)\n",
    "  print(\"Cuda torch working : \",end=\"\")\n",
    "  print(templist.is_cuda)\n",
    "  print(\"current device no. : \",end=\"\")\n",
    "  print(torch.cuda.current_device())\n",
    "  print(\"GPU device count : \",end=\"\")\n",
    "  print(torch.cuda.device_count())\n",
    "  print(\"GPU name : \",end=\"\")\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  print(\"device : \",device)\n",
    "elif torch.backends.mps.is_available() == True:\n",
    "  print(\"Apple device detected\\nActivating Apple Silicon GPU\")\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  print(\"cant use gpu , activating cpu\")\n",
    "  device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:42<00:00, 974.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At learning rate 1.0E-03  Accrucary : 58.51  Mean difference 0.80 Loss : 1.49616\n"
     ]
    }
   ],
   "source": [
    "##ML1\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        drop_prob = 0.2 # 0.2 == 20%\n",
    "        self.fc1 = nn.Linear(8, 16,bias=True)\n",
    "        self.fc2 = nn.Linear(16, 32,bias=True)\n",
    "        self.fc3 = nn.Linear(32, 64,bias=True)\n",
    "        self.fc4 = nn.Linear(64, 100)\n",
    "        self.relu = torch.nn.Sigmoid()\n",
    "        self.dropout = torch.nn.Dropout(p=drop_prob)# nn.Dropout module is a regularization technique for reducing overfitting in neural networks by randomly setting a proportion of input units to 0 during training\n",
    "        self.reset_weights()\n",
    "        \n",
    "        ## initialize weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "net = Net()\n",
    "net=net.to(device)\n",
    "\n",
    "#lr_list = [10000,100,1,1e-3,1e-5,1e-10]\n",
    "lr_list = [1e-3]\n",
    "\n",
    "for LR in lr_list:\n",
    "    net.reset_weights()##reset weight and bias\n",
    "    \n",
    "    #Loss Model\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #Optimizer Model\n",
    "    optimizer = optim.SGD(net.parameters(), lr=LR)#giving only a few random dataset to feed to learn(it speets up)\n",
    "    #Repeat\n",
    "    nb_epoch=100000\n",
    "    #Input data turn into Torch\n",
    "    X = torch.FloatTensor(data_train)\n",
    "    #print(X.shape) // [9999, 8]\n",
    "    X = X.to(device)\n",
    "    Y = torch.FloatTensor(data_y)\n",
    "    #print(Y.shape) // [9999, 98]\n",
    "    Y = Y.to(device)\n",
    "\n",
    "\n",
    "    \"\"\"#checking if Y is fine\n",
    "    for i in range(len(data_y)):\n",
    "        print(data_y[i].index(max(data_y[i])))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #ML\n",
    "    from tqdm import tqdm\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    ##Evaluate\n",
    "    print(\"At learning rate %.1E\"%LR,end=\"  \")##print learning rate\n",
    "    ## accruacy\n",
    "    sum = 0\n",
    "    for i in (range(len(Y))): # range : 9999\n",
    "        if torch.argmax(outputs[i]) == torch.argmax(Y[i]):\n",
    "            sum = sum + 1\n",
    "    print(\"Accrucary : %.2f\"%(sum/len(Y)*100),end=\"  \")\n",
    "    ## avrage cost\n",
    "    sum = 0\n",
    "    for i in (range(len(Y))): # range : 9999\n",
    "        sum = torch.argmax(outputs[i]) - torch.argmax(Y[i]) + sum\n",
    "    print(\"Mean difference %.2f\"%(sum.item()/len(Y)),end=\" \")\n",
    "    print(\"Loss : %.5f\"%loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "At learning rate 1.0E+04  Accrucary : 49.85  Mean difference -18.04  \n",
    "At learning rate 1.0E+02  Accrucary : 49.85  Mean difference -18.04  \n",
    "At learning rate 1.0E+00  Accrucary : 49.85  Mean difference -18.04\n",
    "##### At learning rate 1.0E-03  Accrucary : 56.82  Mean difference 0.01\n",
    "At learning rate 1.0E-05  Accrucary : 50.19  Mean difference 2.05  \n",
    "At learning rate 1.0E-10  Accrucary : 0.51  Mean difference 28.17  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        drop_prob = 0.1 # 0.2 == 20%\n",
    "        self.fc1 = nn.Linear(8, 256,bias=True)\n",
    "        self.fc2 = nn.Linear(256, 64,bias=True)\n",
    "        self.fc3 = nn.Linear(64, 16,bias=True)\n",
    "        self.fc4 = nn.Linear(16, 1,bias=True)\n",
    "        self.relu = torch.nn.Sigmoid()\n",
    "        self.dropout = torch.nn.Dropout(p=drop_prob)# nn.Dropout module is a regularization technique for reducing overfitting in neural networks by randomly setting a proportion of input units to 0 during training\n",
    "        self.reset_weights()\n",
    "        \n",
    "        ## initialize weight\n",
    "    \"\"\"#with out dropout\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "net = Net()\n",
    "net=net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std =StandardScaler()\n",
    "X_train_std = std.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data turn into Torch\n",
    "X = torch.FloatTensor(X_train_std)\n",
    "#print(X.shape)# // [9999, 8]\n",
    "X = X.to(device)\n",
    "Y = torch.FloatTensor(data_y_original)\n",
    "#print(Y.shape)# // [9999]\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:09<01:29,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  585976.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:19<01:19,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  182231.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:29<01:09,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  167782.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:39<00:58,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  146503.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:49<00:49,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  136636.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:59<00:40, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  135747.328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:09<00:30, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  131750.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:19<00:20, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  127880.140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:29<00:10, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  126888.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:40<00:00, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost :  126032.171875\n",
      "LR : 1E-03\n",
      "sum cost : 126032.172At learning rate 1.0E-03  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accrucary : 58.22  Mean difference 1.44 Loss : 4.3608\n"
     ]
    }
   ],
   "source": [
    "##ML2\n",
    "\n",
    "###################################################################SET VALUE\n",
    "#lr_list = [10000,100,1,1e-3,1e-5,1e-10]\n",
    "lr_list = [0.001]\n",
    "nb_epoch=10\n",
    "####################################################################\n",
    "print(\"current device :\",device)\n",
    "for LR in lr_list: ## ML2\n",
    "    net.reset_weights()##reset weight and bias\n",
    "    \n",
    "    #Loss Model\n",
    "    loss = torch.nn.MSELoss().to(device)\n",
    "    #Optimizer Model\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=LR)\n",
    "    #Repeat\n",
    "    \n",
    "    \n",
    "    \n",
    "    #ML\n",
    "    net.train()\n",
    "    from tqdm import tqdm\n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        sum_cost = 0\n",
    "        for x, y in zip(X,Y):\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            hypothesis = net(x)\n",
    "            cost = loss(hypothesis, y)\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "            sum_cost = sum_cost + cost \n",
    "            #print(x,y)\n",
    "        print(\"cost : \",sum_cost.item())\n",
    "        \n",
    "    ##Evaluate\n",
    "    net.eval()\n",
    "    hypothesis = net(X)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        print(\"LR : %.0E\"%LR)\n",
    "        print(\"sum cost : %.3f\"%sum_cost.item(),end=\"\")\n",
    "        print(\"At learning rate :  %.1E\"%LR,end=\"  \")##print learning rate\n",
    "        ## accruacy\n",
    "        sum = 0\n",
    "        for i in (range(len(Y))): # range : 9999\n",
    "            if (int(hypothesis[i][0].item())) == (int(Y[i][0].item())):\n",
    "                sum = sum + 1\n",
    "        print(\"Accrucary : %.2f\"%(sum/len(Y)*100),end=\"  \")\n",
    "        ## avrage cost\n",
    "        sum = 0\n",
    "        for i in (range(len(Y))): # range : 9999\n",
    "            sum = (hypothesis[i][0]) - (Y[i][0]) + sum\n",
    "        print(\"Mean difference %.2f\"%(sum.item()/len(Y)),end=\" \")\n",
    "        print(\"Loss : %.4f\"%cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR : 1E-03\n",
      "sum cost : 128473.242At learning rate 1.0E-03  Accrucary : 49.56  Mean difference 2.28 Loss : 21.0292\n"
     ]
    }
   ],
   "source": [
    "##Evaluate\n",
    "net.eval()\n",
    "hypothesis = net(X)\n",
    "with torch.no_grad():\n",
    "\n",
    "    print(\"LR : %.0E\"%LR)\n",
    "    print(\"sum cost : %.3f\"%sum_cost.item(),end=\"\")\n",
    "    print(\"At learning rate %.1E\"%LR,end=\"  \")##print learning rate\n",
    "    ## accruacy\n",
    "    sum = 0\n",
    "    for i in (range(len(Y))): # range : 9999\n",
    "        if (int(hypothesis[i][0].item())) == (int(Y[i][0].item())):\n",
    "            sum = sum + 1\n",
    "    print(\"Accrucary : %.2f\"%(sum/len(Y)*100),end=\"  \")\n",
    "    ## avrage cost\n",
    "    sum = 0\n",
    "    for i in (range(len(Y))): # range : 9999\n",
    "        sum = (hypothesis[i][0]) - (Y[i][0]) + sum\n",
    "    print(\"Mean difference %.2f\"%(sum.item()/len(Y)),end=\" \")\n",
    "    print(\"Loss : %.4f\"%cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0166],\n",
       "        [ 0.9196],\n",
       "        [ 1.0475],\n",
       "        ...,\n",
       "        [15.7968],\n",
       "        [24.2621],\n",
       "        [33.6528]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR : 1E-03  \n",
    "sum cost :  6393408.5At learning rate 1.0E-03  Accrucary : 0.00  Mean difference -0.21 Loss : 176.47638"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END\n",
    "\n",
    "2023/01/24\n",
    "\n",
    "0152i\n",
    "\n",
    "it took about... 2 hours  \n",
    "I am goin g to sleep now  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
