{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Rice price predection by ML\n",
    "\n",
    "2023/01/13 1633i I didnt do anything yet\n",
    "And I dont want to\n",
    "\n",
    "to make this we need to use multivariate liner regression\n",
    "\n",
    "lets get init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the price data as dim = 2 (with out yearly avrage)\n",
    "I made the code that you dont have to change the original scv file so this code could be used from other people and its easyer for me if I have to use it later\n",
    "\n",
    "the data tracks from 1996 bacuase that the oldest data from the rice price (kamis.or.kr)\n",
    "and we tracked untill 2021 because since we have to PREDICT we are going to predic the 2022 rice price\n",
    "lets see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data count : 312\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "#f = open(\"C:/Users\\OWO\\Documents\\AA_CODE\\@Projects\\Projects\\ai_0001_lr_rice price track\\rice price data.xls\")\n",
    "f = open(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0001_lr_rice price track/rice price data.csv\",encoding='UTF-8')\n",
    "read = csv.reader(f)\n",
    "\n",
    "#getting data from the csv file\n",
    "for a in read:\n",
    "  data.append(a)\n",
    "#deleting the first row because thats the legend\n",
    "del data[0]\n",
    "\n",
    "#making the price list in a dim = 1 list\n",
    "pricelist = []\n",
    "\n",
    "for year in range(0,26):\n",
    "  for month in range(0,14):\n",
    "    templist = []#temp list is for the structure (dimenstion) of the list since we need n,1 structued list to use for the tensor\n",
    "    if (month > 0 and month < 13):\n",
    "      templist.append(float(data[year][month].replace(\",\",'')))#since the origianl data is money it has , and its str so we have to remove \",\" and make it a float (it was hard)\n",
    "      pricelist.append((templist))\n",
    "      templist = []\n",
    "\n",
    "f.close\n",
    "#print(pricelist)\n",
    "print(\"total data count : \"+str(len(pricelist)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "f = open(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0001_lr_rice price track/statisticsDivision_20230113161645.csv\",encoding='cp949')\n",
    "read = csv.reader(f)\n",
    "\n",
    "#getting data from the csv file\n",
    "for a in read:\n",
    "  data.append(a)\n",
    "#deleting the first row because thats the legend\n",
    "for i in range(0,10):#deleting 10 (unnecessary row) from it\n",
    "  del data[0]\n",
    "\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "#list datas\n",
    "\n",
    "\n",
    "##year list dim 2\n",
    "year_list = []\n",
    "templist = []\n",
    "for i in range(0,26):\n",
    "  for month_list in range(0,12):\n",
    "    templist.append(1996+i)\n",
    "    year_list.append(templist)\n",
    "    templist = []\n",
    "#print(year_list)\n",
    "\n",
    "\n",
    "##month list dim 2\n",
    "month_list = []\n",
    "for i in range(0,26):\n",
    "  for mon in range(0,12):\n",
    "    templist.append(mon+1)\n",
    "    month_list.append(templist)\n",
    "    templist = []\n",
    "#print(month_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##other lists\n",
    "avr_temp = []\n",
    "max_temp = []\n",
    "min_temp = []\n",
    "aor = []#it means amount of rainfall\n",
    "templist = []\n",
    "\n",
    "#print(data)\n",
    "count = 0\n",
    "for year in range(0,26):\n",
    "  for month in range (0,12):\n",
    "  \n",
    "    templist.append(float(data[count][1]))\n",
    "    avr_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][2]))\n",
    "    max_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][3]))\n",
    "    min_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][4]))\n",
    "    aor.append(templist)\n",
    "    templist = []\n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "## test seciton\n",
    "#print(avr_temp)\n",
    "#print(max_temp)\n",
    "#print(min_temp)\n",
    "#print(aor)\n",
    "\n",
    "f.close"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the actual ML section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "torch.Size([312, 1])\n",
      "Epoch    0/100000 w1 : 0.181220 w2 : 0.000588 w3 : 0.001159 w4 : 0.001648 w5 : 0.000739 w6 : 0.010293 b : 0.000090 Cost : 2057303552\n",
      "Epoch  100/100000 w1 : 12.535669 w2 : 0.040741 w3 : 0.080206 w4 : 0.113969 w5 : 0.051128 w6 : 0.711418 b : 0.006240 Cost : 422609504\n",
      "Epoch  200/100000 w1 : 18.016088 w2 : 0.058643 w3 : 0.115270 w4 : 0.163797 w5 : 0.073479 w6 : 1.021356 b : 0.008967 Cost : 100942272\n",
      "Epoch  300/100000 w1 : 20.447220 w2 : 0.066674 w3 : 0.130823 w4 : 0.185903 w5 : 0.083392 w6 : 1.157769 b : 0.010175 Cost : 37646396\n",
      "Epoch  400/100000 w1 : 21.525709 w2 : 0.070327 w3 : 0.137721 w4 : 0.195712 w5 : 0.087789 w6 : 1.217209 b : 0.010711 Cost : 25191364\n",
      "Epoch  500/100000 w1 : 22.004183 w2 : 0.072038 w3 : 0.140781 w4 : 0.200066 w5 : 0.089738 w6 : 1.242507 b : 0.010947 Cost : 22740476\n",
      "Epoch  600/100000 w1 : 22.216488 w2 : 0.072887 w3 : 0.142138 w4 : 0.202001 w5 : 0.090602 w6 : 1.252664 b : 0.011051 Cost : 22258178\n",
      "Epoch  700/100000 w1 : 22.310730 w2 : 0.073354 w3 : 0.142740 w4 : 0.202863 w5 : 0.090985 w6 : 1.256106 b : 0.011097 Cost : 22163236\n",
      "Epoch  800/100000 w1 : 22.352592 w2 : 0.073652 w3 : 0.143007 w4 : 0.203249 w5 : 0.091155 w6 : 1.256572 b : 0.011116 Cost : 22144526\n",
      "Epoch  900/100000 w1 : 22.371231 w2 : 0.073874 w3 : 0.143125 w4 : 0.203424 w5 : 0.091230 w6 : 1.255719 b : 0.011124 Cost : 22140810\n",
      "Epoch 1000/100000 w1 : 22.379551 w2 : 0.074063 w3 : 0.143178 w4 : 0.203505 w5 : 0.091263 w6 : 1.254285 b : 0.011126 Cost : 22140050\n",
      "Epoch 1100/100000 w1 : 22.383301 w2 : 0.074237 w3 : 0.143202 w4 : 0.203545 w5 : 0.091278 w6 : 1.252595 b : 0.011126 Cost : 22139868\n",
      "Epoch 1200/100000 w1 : 22.385023 w2 : 0.074405 w3 : 0.143213 w4 : 0.203567 w5 : 0.091285 w6 : 1.250794 b : 0.011126 Cost : 22139810\n",
      "Epoch 1300/100000 w1 : 22.385843 w2 : 0.074569 w3 : 0.143218 w4 : 0.203581 w5 : 0.091288 w6 : 1.248946 b : 0.011125 Cost : 22139764\n",
      "Epoch 1400/100000 w1 : 22.386276 w2 : 0.074733 w3 : 0.143222 w4 : 0.203592 w5 : 0.091290 w6 : 1.247080 b : 0.011123 Cost : 22139734\n",
      "Epoch 1500/100000 w1 : 22.386490 w2 : 0.074896 w3 : 0.143225 w4 : 0.203601 w5 : 0.091292 w6 : 1.245208 b : 0.011122 Cost : 22139690\n",
      "Epoch 1600/100000 w1 : 22.386681 w2 : 0.075059 w3 : 0.143228 w4 : 0.203610 w5 : 0.091293 w6 : 1.243336 b : 0.011120 Cost : 22139662\n",
      "Epoch 1700/100000 w1 : 22.386860 w2 : 0.075221 w3 : 0.143229 w4 : 0.203619 w5 : 0.091295 w6 : 1.241465 b : 0.011119 Cost : 22139624\n",
      "Epoch 1800/100000 w1 : 22.386967 w2 : 0.075384 w3 : 0.143231 w4 : 0.203628 w5 : 0.091297 w6 : 1.239596 b : 0.011117 Cost : 22139594\n",
      "Epoch 1900/100000 w1 : 22.387072 w2 : 0.075546 w3 : 0.143234 w4 : 0.203637 w5 : 0.091299 w6 : 1.237736 b : 0.011115 Cost : 22139552\n",
      "Epoch 2000/100000 w1 : 22.387177 w2 : 0.075708 w3 : 0.143237 w4 : 0.203646 w5 : 0.091302 w6 : 1.235876 b : 0.011114 Cost : 22139516\n",
      "Epoch 2100/100000 w1 : 22.387280 w2 : 0.075871 w3 : 0.143240 w4 : 0.203655 w5 : 0.091305 w6 : 1.234022 b : 0.011112 Cost : 22139484\n",
      "Epoch 2200/100000 w1 : 22.387384 w2 : 0.076033 w3 : 0.143243 w4 : 0.203664 w5 : 0.091308 w6 : 1.232175 b : 0.011111 Cost : 22139454\n",
      "Epoch 2300/100000 w1 : 22.387487 w2 : 0.076196 w3 : 0.143246 w4 : 0.203674 w5 : 0.091311 w6 : 1.230327 b : 0.011109 Cost : 22139422\n",
      "Epoch 2400/100000 w1 : 22.387592 w2 : 0.076358 w3 : 0.143250 w4 : 0.203684 w5 : 0.091315 w6 : 1.228488 b : 0.011108 Cost : 22139378\n",
      "Epoch 2500/100000 w1 : 22.387695 w2 : 0.076521 w3 : 0.143255 w4 : 0.203695 w5 : 0.091319 w6 : 1.226652 b : 0.011106 Cost : 22139346\n",
      "Epoch 2600/100000 w1 : 22.387798 w2 : 0.076683 w3 : 0.143259 w4 : 0.203705 w5 : 0.091323 w6 : 1.224816 b : 0.011104 Cost : 22139306\n",
      "Epoch 2700/100000 w1 : 22.387901 w2 : 0.076845 w3 : 0.143264 w4 : 0.203715 w5 : 0.091327 w6 : 1.222993 b : 0.011103 Cost : 22139276\n",
      "Epoch 2800/100000 w1 : 22.388004 w2 : 0.077008 w3 : 0.143268 w4 : 0.203726 w5 : 0.091332 w6 : 1.221169 b : 0.011101 Cost : 22139242\n",
      "Epoch 2900/100000 w1 : 22.388107 w2 : 0.077170 w3 : 0.143273 w4 : 0.203737 w5 : 0.091337 w6 : 1.219348 b : 0.011100 Cost : 22139214\n",
      "Epoch 3000/100000 w1 : 22.388210 w2 : 0.077333 w3 : 0.143278 w4 : 0.203749 w5 : 0.091342 w6 : 1.217536 b : 0.011098 Cost : 22139180\n",
      "Epoch 3100/100000 w1 : 22.388311 w2 : 0.077495 w3 : 0.143284 w4 : 0.203761 w5 : 0.091348 w6 : 1.215724 b : 0.011096 Cost : 22139146\n",
      "Epoch 3200/100000 w1 : 22.388414 w2 : 0.077658 w3 : 0.143290 w4 : 0.203773 w5 : 0.091354 w6 : 1.213917 b : 0.011095 Cost : 22139114\n",
      "Epoch 3300/100000 w1 : 22.388515 w2 : 0.077820 w3 : 0.143296 w4 : 0.203785 w5 : 0.091360 w6 : 1.212117 b : 0.011093 Cost : 22139078\n",
      "Epoch 3400/100000 w1 : 22.388617 w2 : 0.077982 w3 : 0.143302 w4 : 0.203796 w5 : 0.091367 w6 : 1.210317 b : 0.011092 Cost : 22139048\n",
      "Epoch 3500/100000 w1 : 22.388718 w2 : 0.078145 w3 : 0.143308 w4 : 0.203808 w5 : 0.091373 w6 : 1.208525 b : 0.011090 Cost : 22139012\n",
      "Epoch 3600/100000 w1 : 22.388819 w2 : 0.078308 w3 : 0.143315 w4 : 0.203822 w5 : 0.091380 w6 : 1.206737 b : 0.011089 Cost : 22138982\n",
      "Epoch 3700/100000 w1 : 22.388920 w2 : 0.078472 w3 : 0.143323 w4 : 0.203835 w5 : 0.091387 w6 : 1.204948 b : 0.011087 Cost : 22138946\n",
      "Epoch 3800/100000 w1 : 22.389019 w2 : 0.078635 w3 : 0.143330 w4 : 0.203848 w5 : 0.091395 w6 : 1.203170 b : 0.011085 Cost : 22138914\n",
      "Epoch 3900/100000 w1 : 22.389120 w2 : 0.078798 w3 : 0.143338 w4 : 0.203862 w5 : 0.091403 w6 : 1.201394 b : 0.011084 Cost : 22138884\n",
      "Epoch 4000/100000 w1 : 22.389219 w2 : 0.078961 w3 : 0.143345 w4 : 0.203875 w5 : 0.091411 w6 : 1.199618 b : 0.011082 Cost : 22138854\n",
      "Epoch 4100/100000 w1 : 22.389318 w2 : 0.079124 w3 : 0.143353 w4 : 0.203889 w5 : 0.091419 w6 : 1.197853 b : 0.011081 Cost : 22138820\n",
      "Epoch 4200/100000 w1 : 22.389418 w2 : 0.079287 w3 : 0.143361 w4 : 0.203903 w5 : 0.091428 w6 : 1.196089 b : 0.011079 Cost : 22138790\n",
      "Epoch 4300/100000 w1 : 22.389517 w2 : 0.079451 w3 : 0.143370 w4 : 0.203918 w5 : 0.091436 w6 : 1.194326 b : 0.011077 Cost : 22138756\n",
      "Epoch 4400/100000 w1 : 22.389616 w2 : 0.079614 w3 : 0.143379 w4 : 0.203933 w5 : 0.091445 w6 : 1.192574 b : 0.011076 Cost : 22138730\n",
      "Epoch 4500/100000 w1 : 22.389715 w2 : 0.079777 w3 : 0.143388 w4 : 0.203947 w5 : 0.091455 w6 : 1.190821 b : 0.011074 Cost : 22138696\n",
      "Epoch 4600/100000 w1 : 22.389812 w2 : 0.079940 w3 : 0.143397 w4 : 0.203962 w5 : 0.091465 w6 : 1.189072 b : 0.011073 Cost : 22138664\n",
      "Epoch 4700/100000 w1 : 22.389912 w2 : 0.080103 w3 : 0.143406 w4 : 0.203977 w5 : 0.091474 w6 : 1.187331 b : 0.011071 Cost : 22138638\n",
      "Epoch 4800/100000 w1 : 22.390009 w2 : 0.080266 w3 : 0.143416 w4 : 0.203992 w5 : 0.091485 w6 : 1.185591 b : 0.011070 Cost : 22138606\n",
      "Epoch 4900/100000 w1 : 22.390106 w2 : 0.080430 w3 : 0.143426 w4 : 0.204008 w5 : 0.091495 w6 : 1.183855 b : 0.011068 Cost : 22138574\n",
      "Epoch 5000/100000 w1 : 22.390203 w2 : 0.080593 w3 : 0.143437 w4 : 0.204024 w5 : 0.091506 w6 : 1.182126 b : 0.011066 Cost : 22138544\n",
      "Epoch 5100/100000 w1 : 22.390301 w2 : 0.080756 w3 : 0.143447 w4 : 0.204041 w5 : 0.091517 w6 : 1.180398 b : 0.011065 Cost : 22138518\n",
      "Epoch 5200/100000 w1 : 22.390398 w2 : 0.080919 w3 : 0.143458 w4 : 0.204057 w5 : 0.091528 w6 : 1.178674 b : 0.011063 Cost : 22138480\n",
      "Epoch 5300/100000 w1 : 22.390495 w2 : 0.081082 w3 : 0.143468 w4 : 0.204074 w5 : 0.091539 w6 : 1.176957 b : 0.011062 Cost : 22138450\n",
      "Epoch 5400/100000 w1 : 22.390593 w2 : 0.081245 w3 : 0.143479 w4 : 0.204090 w5 : 0.091551 w6 : 1.175241 b : 0.011060 Cost : 22138422\n",
      "Epoch 5500/100000 w1 : 22.390688 w2 : 0.081409 w3 : 0.143491 w4 : 0.204106 w5 : 0.091563 w6 : 1.173530 b : 0.011058 Cost : 22138388\n",
      "Epoch 5600/100000 w1 : 22.390783 w2 : 0.081572 w3 : 0.143503 w4 : 0.204124 w5 : 0.091575 w6 : 1.171825 b : 0.011057 Cost : 22138362\n",
      "Epoch 5700/100000 w1 : 22.390881 w2 : 0.081735 w3 : 0.143515 w4 : 0.204142 w5 : 0.091588 w6 : 1.170121 b : 0.011055 Cost : 22138338\n",
      "Epoch 5800/100000 w1 : 22.390974 w2 : 0.081898 w3 : 0.143527 w4 : 0.204160 w5 : 0.091601 w6 : 1.168422 b : 0.011054 Cost : 22138306\n",
      "Epoch 5900/100000 w1 : 22.391071 w2 : 0.082061 w3 : 0.143539 w4 : 0.204178 w5 : 0.091614 w6 : 1.166729 b : 0.011052 Cost : 22138272\n",
      "Epoch 6000/100000 w1 : 22.391167 w2 : 0.082224 w3 : 0.143551 w4 : 0.204196 w5 : 0.091627 w6 : 1.165037 b : 0.011051 Cost : 22138250\n",
      "Epoch 6100/100000 w1 : 22.391260 w2 : 0.082388 w3 : 0.143564 w4 : 0.204214 w5 : 0.091640 w6 : 1.163350 b : 0.011049 Cost : 22138220\n",
      "Epoch 6200/100000 w1 : 22.391356 w2 : 0.082551 w3 : 0.143578 w4 : 0.204232 w5 : 0.091654 w6 : 1.161669 b : 0.011047 Cost : 22138188\n",
      "Epoch 6300/100000 w1 : 22.391449 w2 : 0.082714 w3 : 0.143591 w4 : 0.204251 w5 : 0.091668 w6 : 1.159989 b : 0.011046 Cost : 22138162\n",
      "Epoch 6400/100000 w1 : 22.391544 w2 : 0.082877 w3 : 0.143605 w4 : 0.204270 w5 : 0.091683 w6 : 1.158314 b : 0.011044 Cost : 22138136\n",
      "Epoch 6500/100000 w1 : 22.391638 w2 : 0.083040 w3 : 0.143618 w4 : 0.204290 w5 : 0.091697 w6 : 1.156645 b : 0.011043 Cost : 22138108\n",
      "Epoch 6600/100000 w1 : 22.391731 w2 : 0.083204 w3 : 0.143632 w4 : 0.204309 w5 : 0.091712 w6 : 1.154976 b : 0.011041 Cost : 22138076\n",
      "Epoch 6700/100000 w1 : 22.391825 w2 : 0.083367 w3 : 0.143646 w4 : 0.204328 w5 : 0.091727 w6 : 1.153313 b : 0.011039 Cost : 22138044\n",
      "Epoch 6800/100000 w1 : 22.391918 w2 : 0.083530 w3 : 0.143661 w4 : 0.204348 w5 : 0.091742 w6 : 1.151656 b : 0.011038 Cost : 22138020\n",
      "Epoch 6900/100000 w1 : 22.392012 w2 : 0.083693 w3 : 0.143676 w4 : 0.204367 w5 : 0.091758 w6 : 1.149999 b : 0.011036 Cost : 22137998\n",
      "Epoch 7000/100000 w1 : 22.392105 w2 : 0.083856 w3 : 0.143691 w4 : 0.204388 w5 : 0.091774 w6 : 1.148348 b : 0.011035 Cost : 22137962\n",
      "Epoch 7100/100000 w1 : 22.392197 w2 : 0.084019 w3 : 0.143706 w4 : 0.204409 w5 : 0.091790 w6 : 1.146703 b : 0.011033 Cost : 22137940\n",
      "Epoch 7200/100000 w1 : 22.392288 w2 : 0.084183 w3 : 0.143720 w4 : 0.204430 w5 : 0.091806 w6 : 1.145058 b : 0.011032 Cost : 22137906\n",
      "Epoch 7300/100000 w1 : 22.392382 w2 : 0.084346 w3 : 0.143736 w4 : 0.204451 w5 : 0.091823 w6 : 1.143417 b : 0.011030 Cost : 22137886\n",
      "Epoch 7400/100000 w1 : 22.392473 w2 : 0.084509 w3 : 0.143752 w4 : 0.204472 w5 : 0.091840 w6 : 1.141784 b : 0.011028 Cost : 22137856\n",
      "Epoch 7500/100000 w1 : 22.392565 w2 : 0.084672 w3 : 0.143768 w4 : 0.204492 w5 : 0.091857 w6 : 1.140151 b : 0.011027 Cost : 22137828\n",
      "Epoch 7600/100000 w1 : 22.392656 w2 : 0.084835 w3 : 0.143785 w4 : 0.204513 w5 : 0.091874 w6 : 1.138521 b : 0.011025 Cost : 22137804\n",
      "Epoch 7700/100000 w1 : 22.392748 w2 : 0.084998 w3 : 0.143801 w4 : 0.204536 w5 : 0.091891 w6 : 1.136900 b : 0.011024 Cost : 22137776\n",
      "Epoch 7800/100000 w1 : 22.392838 w2 : 0.085162 w3 : 0.143818 w4 : 0.204558 w5 : 0.091909 w6 : 1.135279 b : 0.011022 Cost : 22137746\n",
      "Epoch 7900/100000 w1 : 22.392929 w2 : 0.085325 w3 : 0.143834 w4 : 0.204580 w5 : 0.091927 w6 : 1.133659 b : 0.011020 Cost : 22137724\n",
      "Epoch 8000/100000 w1 : 22.393019 w2 : 0.085488 w3 : 0.143851 w4 : 0.204603 w5 : 0.091946 w6 : 1.132050 b : 0.011019 Cost : 22137696\n",
      "Epoch 8100/100000 w1 : 22.393110 w2 : 0.085652 w3 : 0.143869 w4 : 0.204625 w5 : 0.091964 w6 : 1.130441 b : 0.011017 Cost : 22137670\n",
      "Epoch 8200/100000 w1 : 22.393200 w2 : 0.085816 w3 : 0.143887 w4 : 0.204647 w5 : 0.091983 w6 : 1.128832 b : 0.011016 Cost : 22137642\n",
      "Epoch 8300/100000 w1 : 22.393290 w2 : 0.085980 w3 : 0.143905 w4 : 0.204670 w5 : 0.092002 w6 : 1.127235 b : 0.011014 Cost : 22137616\n",
      "Epoch 8400/100000 w1 : 22.393381 w2 : 0.086144 w3 : 0.143923 w4 : 0.204693 w5 : 0.092021 w6 : 1.125637 b : 0.011013 Cost : 22137590\n",
      "Epoch 8500/100000 w1 : 22.393469 w2 : 0.086308 w3 : 0.143941 w4 : 0.204717 w5 : 0.092041 w6 : 1.124040 b : 0.011011 Cost : 22137568\n",
      "Epoch 8600/100000 w1 : 22.393559 w2 : 0.086472 w3 : 0.143958 w4 : 0.204741 w5 : 0.092060 w6 : 1.122453 b : 0.011009 Cost : 22137542\n",
      "Epoch 8700/100000 w1 : 22.393648 w2 : 0.086636 w3 : 0.143978 w4 : 0.204765 w5 : 0.092080 w6 : 1.120867 b : 0.011008 Cost : 22137516\n",
      "Epoch 8800/100000 w1 : 22.393736 w2 : 0.086800 w3 : 0.143997 w4 : 0.204788 w5 : 0.092101 w6 : 1.119282 b : 0.011006 Cost : 22137492\n",
      "Epoch 8900/100000 w1 : 22.393826 w2 : 0.086964 w3 : 0.144016 w4 : 0.204812 w5 : 0.092121 w6 : 1.117705 b : 0.011005 Cost : 22137468\n",
      "Epoch 9000/100000 w1 : 22.393913 w2 : 0.087128 w3 : 0.144036 w4 : 0.204836 w5 : 0.092142 w6 : 1.116131 b : 0.011003 Cost : 22137440\n",
      "Epoch 9100/100000 w1 : 22.394003 w2 : 0.087292 w3 : 0.144055 w4 : 0.204860 w5 : 0.092163 w6 : 1.114558 b : 0.011001 Cost : 22137412\n",
      "Epoch 9200/100000 w1 : 22.394091 w2 : 0.087455 w3 : 0.144074 w4 : 0.204886 w5 : 0.092184 w6 : 1.112990 b : 0.011000 Cost : 22137394\n",
      "Epoch 9300/100000 w1 : 22.394178 w2 : 0.087619 w3 : 0.144094 w4 : 0.204911 w5 : 0.092205 w6 : 1.111428 b : 0.010998 Cost : 22137364\n",
      "Epoch 9400/100000 w1 : 22.394266 w2 : 0.087783 w3 : 0.144115 w4 : 0.204936 w5 : 0.092227 w6 : 1.109866 b : 0.010997 Cost : 22137340\n",
      "Epoch 9500/100000 w1 : 22.394352 w2 : 0.087947 w3 : 0.144135 w4 : 0.204962 w5 : 0.092249 w6 : 1.108307 b : 0.010995 Cost : 22137316\n",
      "Epoch 9600/100000 w1 : 22.394440 w2 : 0.088111 w3 : 0.144156 w4 : 0.204987 w5 : 0.092271 w6 : 1.106758 b : 0.010994 Cost : 22137286\n",
      "Epoch 9700/100000 w1 : 22.394527 w2 : 0.088275 w3 : 0.144177 w4 : 0.205012 w5 : 0.092293 w6 : 1.105208 b : 0.010992 Cost : 22137270\n",
      "Epoch 9800/100000 w1 : 22.394613 w2 : 0.088439 w3 : 0.144198 w4 : 0.205038 w5 : 0.092316 w6 : 1.103658 b : 0.010990 Cost : 22137246\n",
      "Epoch 9900/100000 w1 : 22.394701 w2 : 0.088603 w3 : 0.144219 w4 : 0.205064 w5 : 0.092339 w6 : 1.102120 b : 0.010989 Cost : 22137218\n",
      "Epoch 10000/100000 w1 : 22.394785 w2 : 0.088767 w3 : 0.144240 w4 : 0.205091 w5 : 0.092362 w6 : 1.100582 b : 0.010987 Cost : 22137196\n",
      "Epoch 10100/100000 w1 : 22.394873 w2 : 0.088931 w3 : 0.144262 w4 : 0.205118 w5 : 0.092385 w6 : 1.099045 b : 0.010986 Cost : 22137170\n",
      "Epoch 10200/100000 w1 : 22.394958 w2 : 0.089095 w3 : 0.144284 w4 : 0.205145 w5 : 0.092408 w6 : 1.097515 b : 0.010984 Cost : 22137148\n",
      "Epoch 10300/100000 w1 : 22.395044 w2 : 0.089258 w3 : 0.144307 w4 : 0.205172 w5 : 0.092432 w6 : 1.095989 b : 0.010982 Cost : 22137128\n",
      "Epoch 10400/100000 w1 : 22.395128 w2 : 0.089422 w3 : 0.144329 w4 : 0.205198 w5 : 0.092456 w6 : 1.094463 b : 0.010981 Cost : 22137094\n",
      "Epoch 10500/100000 w1 : 22.395214 w2 : 0.089586 w3 : 0.144352 w4 : 0.205225 w5 : 0.092480 w6 : 1.092942 b : 0.010979 Cost : 22137078\n",
      "Epoch 10600/100000 w1 : 22.395300 w2 : 0.089750 w3 : 0.144374 w4 : 0.205252 w5 : 0.092505 w6 : 1.091428 b : 0.010978 Cost : 22137050\n",
      "Epoch 10700/100000 w1 : 22.395384 w2 : 0.089914 w3 : 0.144396 w4 : 0.205281 w5 : 0.092530 w6 : 1.089914 b : 0.010976 Cost : 22137028\n",
      "Epoch 10800/100000 w1 : 22.395470 w2 : 0.090078 w3 : 0.144420 w4 : 0.205309 w5 : 0.092554 w6 : 1.088401 b : 0.010975 Cost : 22137002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m#impove H(x) with cost\u001b[39;00m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 60\u001b[0m cost\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     61\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "#Data\n",
    "\"\"\"\n",
    "pricelist - y tensor\n",
    "year_list\n",
    "month_list\n",
    "avr_temp\n",
    "max_temp\n",
    "min_temp\n",
    "aor\n",
    "\"\"\"\n",
    "\n",
    "pricelist = torch.FloatTensor(pricelist)\n",
    "year_list = torch.FloatTensor(year_list)\n",
    "month_list = torch.FloatTensor(month_list)\n",
    "avr_temp = torch.FloatTensor(avr_temp)\n",
    "max_temp = torch.FloatTensor(max_temp)\n",
    "min_temp = torch.FloatTensor(min_temp)\n",
    "aor = torch.FloatTensor(aor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#varification\n",
    "\n",
    "print(pricelist.size())\n",
    "print(year_list.size())\n",
    "print(month_list.size())\n",
    "print(avr_temp.size())\n",
    "print(max_temp.size())\n",
    "print(min_temp.size())\n",
    "print(aor.size())\n",
    "\n",
    "\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1,requires_grad=True)\n",
    "w3 = torch.zeros(1,requires_grad=True)\n",
    "w4 = torch.zeros(1,requires_grad=True)\n",
    "w5 = torch.zeros(1,requires_grad=True)\n",
    "w6 = torch.zeros(1,requires_grad=True)\n",
    "b  = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD([w1,w2,w3,w4,w5,w6,b], lr=1e-9) #learning rate\n",
    "\n",
    "nb_epochs = 100000\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "  #hypothesis calcuation\n",
    "  hypothesis = year_list * w1 + month_list * w2 + avr_temp * w3 + max_temp * w4 + min_temp * w5 + aor * w6 + b\n",
    "  \n",
    "  #cost calculation\n",
    "  cost = torch.mean((hypothesis - pricelist)**2)\n",
    "  \n",
    "  #impove H(x) with cost\n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if epoch % 100 == 0:\n",
    "    print(\"Epoch %4d/%d w1 : %f w2 : %f w3 : %f w4 : %f w5 : %f w6 : %f b : %f Cost : %.0f\"%(epoch,nb_epochs,w1.item(),w2.item(),w3.item(),w4.item(),w5.item(),w6.item(),b.item(),cost.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate test\n",
    "test 3 : 100000 // lr = 1e-9  //83515269120\n",
    "test 1 : 100000 // lr = 1e-10 //116692877312\n",
    "test 2 : 100000 // lr = 1e-15 //6046595619487744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR = 1.000e-07 Epoch 100000/100000 w1 :   22.4 w2 : 14.3 w3 :  3.8 w4 :  3.5 w5 :    4.8 w6 :   -0.3 b :  -0.1 Cost : 22104208\n",
      "LR = 2.000e-07 Epoch 100000/100000 w1 :   22.3 w2 : 25.3 w3 :  3.6 w4 :  2.8 w5 :    5.3 w6 :   -0.3 b :  -0.3 Cost : 22092028\n",
      "LR = 1.000e-08 Epoch 100000/100000 w1 :   22.4 w2 :  1.7 w3 :  1.1 w4 :  1.1 w5 :    1.1 w6 :    0.3 b :  -0.0 Cost : 22126492\n"
     ]
    }
   ],
   "source": [
    "elist = [1e-7,2e-7,1e-8]\n",
    "\n",
    "pricelist = torch.FloatTensor(pricelist)\n",
    "year_list = torch.FloatTensor(year_list)\n",
    "month_list = torch.FloatTensor(month_list)\n",
    "avr_temp = torch.FloatTensor(avr_temp)\n",
    "max_temp = torch.FloatTensor(max_temp)\n",
    "min_temp = torch.FloatTensor(min_temp)\n",
    "aor = torch.FloatTensor(aor)\n",
    "\n",
    "\n",
    "\n",
    "for LR in elist:\n",
    "    \n",
    "  w1 = torch.zeros(1, requires_grad=True)\n",
    "  w2 = torch.zeros(1,requires_grad=True)\n",
    "  w3 = torch.zeros(1,requires_grad=True)\n",
    "  w4 = torch.zeros(1,requires_grad=True)\n",
    "  w5 = torch.zeros(1,requires_grad=True)\n",
    "  w6 = torch.zeros(1,requires_grad=True)\n",
    "  b  = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "\n",
    "  optimizer = optim.SGD([w1,w2,w3,w4,w5,w6,b], lr=LR) #learning rate\n",
    "\n",
    "  nb_epochs = 100000\n",
    "\n",
    "  for epoch in range(nb_epochs + 1):\n",
    "    #hypothesis calcuation\n",
    "    hypothesis = year_list * w1 + month_list * w2 + avr_temp * w3 + max_temp * w4 + min_temp * w5 + aor * w6 + b\n",
    "    \n",
    "    #cost calculation\n",
    "    cost = torch.mean((hypothesis - pricelist)**2)\n",
    "    \n",
    "    #impove H(x) with cost\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #if epoch == 100000000:\n",
    "  print(\"LR = %.3e Epoch %d/%d w1 : %6.1f w2 : %4.1f w3 : %4.1f w4 : %4.1f w5 : %6.1f w6 : %6.1f b : %5.1f Cost : %.0f\"%(LR,epoch,nb_epochs,w1.item(),w2.item(),w3.item(),w4.item(),w5.item(),w6.item(),b.item(),cost.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final LR\n",
    "\n",
    "since the 2e-7 has the minimum cost we will go with that model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45446.56999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w1 =   22.3\n",
    "w2 = 25.3\n",
    "w3 =  3.6\n",
    "w4 =  2.8\n",
    "w5 =    5.3\n",
    "w6 =   -0.3\n",
    "b =  -0.3\n",
    "\n",
    "predict_year = float(input(\"type in the year\"))\n",
    "predict_month = float(input(\"type in the month\"))\n",
    "predict_avr = float(input(\"input that month avr temp\"))\n",
    "predict_max = float(input(\"input the max tempreture\"))\n",
    "predict_min = float(input(\"input the minimum tempreture\"))\n",
    "predict_aor = float(input(\"input the amount of rain that month\"))\n",
    "\n",
    "hypothesis = predict_year * w1 + predict_month * w2 + predict_avr * w3 + predict_max * w4 + predict_min * w5 + predict_aor * w6 + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Test\n",
    "\n",
    "lets test with 2022(that we didnt train with) to see how much well it could predict the price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input 2022 price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[53450.0], [52597.0], [52459.0], [52037.0], [51221.0], [50990.0], [50643.0], [49228.0], [48397.0], [49806.0], [53284.0], [51900.0]]\n",
      "total data count : 12\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "#f = open(\"C:/Users\\OWO\\Documents\\AA_CODE\\@Projects\\Projects\\ai_0001_lr_rice price track\\rice price data.xls\")\n",
    "f = open(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0001_lr_rice price track/rice price data.csv\",encoding='UTF-8')\n",
    "read = csv.reader(f)\n",
    "\n",
    "#getting data from the csv file\n",
    "for a in read:\n",
    "  data.append(a)\n",
    "#deleting the first row because thats the legend\n",
    "del data[0]\n",
    "\n",
    "#making the price list in a dim = 1 list\n",
    "predict_pricelist = []\n",
    "\n",
    "for year in range(26,27):\n",
    "  for month in range(0,14):\n",
    "    templist = []#temp list is for the structure (dimenstion) of the list since we need n,1 structued list to use for the tensor\n",
    "    if (month > 0 and month < 13):\n",
    "      templist.append(float(data[year][month].replace(\",\",'')))#since the origianl data is money it has , and its str so we have to remove \",\" and make it a float (it was hard)\n",
    "      predict_pricelist.append((templist))\n",
    "      templist = []\n",
    "\n",
    "f.close\n",
    "print(predict_pricelist)\n",
    "print(\"total data count : \"+str(len(predict_pricelist)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input 2022 input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0], [2048.0]]\n",
      "[[1.0], [2.0], [3.0], [4.0], [5.0], [6.0], [7.0], [8.0], [9.0], [10.0], [11.0], [12.0]]\n",
      "[[-0.3], [0.3], [7.9], [13.8], [18.0], [22.3], [26.0], [25.4], [21.1], [14.2], [10.0], [-0.7]]\n",
      "[[5.3], [5.9], [13.8], [20.4], [24.6], [27.1], [30.3], [29.3], [26.4], [20.2], [16.7], [4.3]]\n",
      "[[-5.3], [-4.9], [2.3], [7.7], [11.7], [18.4], [22.6], [22.3], [16.9], [9.3], [4.4], [-5.2]]\n",
      "[[4.8], [4.7], [92.0], [60.5], [6.1], [180.7], [184.3], [280.8], [157.8], [76.7], [62.3], [17.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "f = open(\"C:/Users/OWO/Documents/AA_CODE/@Projects/Projects/ai_0001_lr_rice price track/statisticsDivision_20230113161645.csv\",encoding='cp949')\n",
    "read = csv.reader(f)\n",
    "\n",
    "#getting data from the csv file\n",
    "for a in read:\n",
    "  data.append(a)\n",
    "#deleting the first row because thats the legend\n",
    "for i in range(0,10):#deleting 10 (unnecessary row) from it\n",
    "  del data[0]\n",
    "\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "#list datas\n",
    "\n",
    "\n",
    "##year list dim 2\n",
    "predict_year_list = []\n",
    "templist = []\n",
    "for i in range(26,27):\n",
    "  for month in range(0,12):\n",
    "    templist.append(2022+i)\n",
    "    predict_year_list.append(templist)\n",
    "    templist = []\n",
    "print(predict_year_list)\n",
    "\n",
    "\n",
    "##month list dim 2\n",
    "predict_month_list = []\n",
    "for i in range(26,27):\n",
    "  for mon in range(0,12):\n",
    "    templist.append(mon+1)\n",
    "    predict_month_list.append(templist)\n",
    "    templist = []\n",
    "print(predict_month_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##other lists\n",
    "predict_avr_temp = []\n",
    "predict_max_temp = []\n",
    "predict_min_temp = []\n",
    "predict_aor = []#it means amount of rainfall\n",
    "templist = []\n",
    "\n",
    "#print(data)\n",
    "count = 312#its 322 but we deleted 10 on the top so... its num - 10\n",
    "for year in range(26,27):\n",
    "  for month in range (0,12):\n",
    "  \n",
    "    templist.append(float(data[count][1]))\n",
    "    predict_avr_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][2]))\n",
    "    predict_max_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][3]))\n",
    "    predict_min_temp.append(templist)\n",
    "    templist = []\n",
    "    templist.append(float(data[count][4]))\n",
    "    predict_aor.append(templist)\n",
    "    templist = []\n",
    "    count = count + 1\n",
    "\n",
    "\n",
    "## test seciton\n",
    "print(predict_avr_temp)\n",
    "print(predict_max_temp)\n",
    "print(predict_min_temp)\n",
    "print(predict_aor)\n",
    "\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-495.7916666666667\n"
     ]
    }
   ],
   "source": [
    "## actual test\n",
    "\n",
    "w1 =   22.3\n",
    "w2 = 25.3\n",
    "w3 =  3.6\n",
    "w4 =  2.8\n",
    "w5 =    5.3\n",
    "w6 =   -0.3\n",
    "b =  -0.3\n",
    "\n",
    "predict_cost_sum = 0\n",
    "\n",
    "for i in range(0,12):\n",
    "  predict_cost_sum = (predict_year_list[i][0] * w1 + predict_month_list[i][0] * w2 + predict_avr_temp[i][0] * w3 + w4 * predict_max_temp[i][0] + w5 * predict_min_temp[i][0] + w6 * predict_aor[i][0] + b)-predict_pricelist[i][0]\n",
    "print(predict_cost_sum/12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is the Korean Rice Price Predictor\n",
      "type in the data then you will recive the price by its data\n",
      " the AI is guessing it would be : 7312 Won\n"
     ]
    }
   ],
   "source": [
    "#data\n",
    "w1 =   22.3\n",
    "w2 = 25.3\n",
    "w3 =  3.6\n",
    "w4 =  2.8\n",
    "w5 =    5.3\n",
    "w6 =   -0.3\n",
    "b =  -0.3\n",
    "\n",
    "\n",
    "print(\"hello this is the Korean Rice Price Predictor\")\n",
    "print(\"type in the data then you will recive the price by its data\")\n",
    "\n",
    "new_year = float(input(\"type in the year : \"))\n",
    "new_month = float(input(\"type in the month : \"))\n",
    "new_avr_temp = float(input(\"type in the average temperature of that month : \"))\n",
    "new_max_temp = float(input(\"type in the maximum temperature of that month : \"))\n",
    "new_min_temp = float(input(\"input the minimum temperture of that month : \"))\n",
    "new_aor = float(input(\"input the amount of rain in that month : \"))\n",
    "\n",
    "print(\" the AI is guessing it would be : \",end=\"\")\n",
    "print(\"%.0f Won\"%(new_year * w1 + new_month * w2 + new_avr_temp * w3 + new_max_temp * w4 + new_min_temp * w5 + new_aor * w6 + b)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the project\n",
    "\n",
    "it's 2023/01/14 \n",
    "\n",
    "I think this took about 4 hours to code\n",
    "it was not very hard but it was a little bit tricky to get the CSV file to list and use that\n",
    "\n",
    "# improvements needed\n",
    "\n",
    "I think I missed a lot of things. I need to research before I choose what data to learn from because I'm not sure if the month is going to make a big effect on the price (I thought so but I think It doesn't at the end) and about rice, you harvest them only once a year (at least in Korea) so I don't it look like the temperature of the month had a lot of effect on the price.\n",
    "And also an important thing is that Most of the time, the price is too cheap than expected. I think it's becuase the data was too varied about the date. Since the economy of Korea grew so fast and rice prices grew fast with it, it couldn't catch up with the new prices, so in this case I should have been tracking (or learning) the last 5 years of data, which (I think) it could have improved the AI\n",
    "\n",
    "# about the learning rate\n",
    "\n",
    "I made a new system for the learning rate. In this one, the learning rate is made on list so you can put many learning rate and print the cost in every learning rate to see what is the most optimized learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dccc2083b900c7e4253a45875410f77045ddb96f227ca83eb40747880cff069"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
