{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MODELS.ai_0014_MARK1 import SoundClassifier_MARK1\n",
    "from MODELS.ai_0014_MARK2 import SoundClassifier_MARK2\n",
    "from MODELS.ai_0014_MARK3 import SoundClassifier_MARK3\n",
    "from GPU_torch import GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPU() missing 1 required positional argument: 'void'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m GPU()\n",
      "\u001b[0;31mTypeError\u001b[0m: GPU() missing 1 required positional argument: 'void'"
     ]
    }
   ],
   "source": [
    "GPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "'''SEED Everything'''\n",
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True # keep True if all the input have same size.\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaning the last line to 10 number since there is total 10 class\n",
    "# Define some parameters\n",
    "num_classes = 10  # for UrbanSound8K dataset, there are 10 classes\n",
    "input_size = 44100 * 4  # based on the fixed length of the audio samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stream...\n",
      "\n",
      "Listening for audio...\n",
      "|----------|"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cafalena/HOUSE/@Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Record audio for a set duration\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m recording \u001b[39m=\u001b[39m sd\u001b[39m.\u001b[39mrec(\u001b[39mint\u001b[39m(duration \u001b[39m*\u001b[39m sample_rate), samplerate\u001b[39m=\u001b[39msample_rate, channels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m sd\u001b[39m.\u001b[39;49mwait()  \u001b[39m# Wait for the recording to finish\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecording finished. Preprocessing and classifying...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Preprocess the recorded audio (depending on how you've implemented UrbanSoundDataset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m#processed_recording = UrbanSoundDataset.preprocess(recording)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cafalena/HOUSE/%40Code/Practice/ai_0014_CNN_Urbansound/ai_0014_Analize.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Preprocess the recorded audio\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mreturn\u001b[39;00m _last_callback\u001b[39m.\u001b[39;49mwait(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/site-packages/sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \n\u001b[1;32m   2597\u001b[0m \u001b[39mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \n\u001b[1;32m   2599\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevent\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   2602\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2603\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torchenv/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# Define the path to the saved model checkpoint\n",
    "saved_model_path = 'best_model.pth'\n",
    "\n",
    "# Create the model\n",
    "model = SoundClassifier_MARK1(input_size, num_classes)\n",
    "\n",
    "# Load the saved model checkpoint\n",
    "model.load_state_dict(torch.load(saved_model_path))\n",
    "model.eval()\n",
    "\n",
    "# Set the duration and sample rate of recordings\n",
    "duration = 5  # seconds\n",
    "sample_rate = 44100  # in hertz, the standard sample rate for UrbanSound8K\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    volume_norm = np.linalg.norm(indata) * 10\n",
    "    print (\"\\r|\" + \"#\" * int(volume_norm) + \"-\" * (10 - int(volume_norm)) + \"|\", end='')\n",
    "\n",
    "# Create a stream object (sound device)\n",
    "stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=sample_rate)\n",
    "\n",
    "# Define the class names\n",
    "class_names = ['air_conditioner', 'car_horn', 'children_playing', 'dog_bark', 'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music']\n",
    "\n",
    "print(\"Starting stream...\")\n",
    "\n",
    "with stream:\n",
    "    while True:\n",
    "        print(\"\\nListening for audio...\")\n",
    "        # Record audio for a set duration\n",
    "        recording = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "        sd.wait()  # Wait for the recording to finish\n",
    "        print(\"Recording finished. Preprocessing and classifying...\")\n",
    "        \n",
    "        # Preprocess the recorded audio (depending on how you've implemented UrbanSoundDataset)\n",
    "        #processed_recording = UrbanSoundDataset.preprocess(recording)\n",
    "\n",
    "        # Preprocess the recorded audio\n",
    "        def preprocess_live_audio(audio):\n",
    "            # Convert the audio to a spectrogram\n",
    "            spectrogram = librosa.feature.melspectrogram(audio, sr=sample_rate)\n",
    "\n",
    "            # Apply the logarithm to the spectrogram\n",
    "            log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "            # Your model might expect a specific shape or other properties, so adjust as needed\n",
    "            # This is a simple example and may not perfectly match your needs\n",
    "\n",
    "            return torch.Tensor(log_spectrogram)\n",
    "\n",
    "        # Replace the preprocessing line in the previous code with the new function\n",
    "        processed_recording = preprocess_live_audio(recording)\n",
    "\n",
    "        # Run the model on the processed audio\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            processed_recording = processed_recording.to(device)\n",
    "            output = model(processed_recording)\n",
    "\n",
    "        # Determine the predicted class\n",
    "        _, predicted_class = torch.max(output.data, 0)\n",
    "        print(f\"Predicted sound class: {class_names[predicted_class.item()]}\")\n",
    "        time.sleep(0.1)  # You can adjust this delay as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
